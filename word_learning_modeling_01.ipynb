{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model I\n",
    "\n",
    "* **Norms**: McRae et al. (2005)\n",
    "* **Model**: Dinu & Lapata (2010) Extension 1\n",
    "    * Topic Modeling for Properties: $p(prop\\mid topic) = \\sum_{word\\in topic}p(word\\mid topic)f(word,prop)$, where $f(word,prop) = \\begin{cases}1 & \\text{if word has prop}\\\\ 0 & otherwise\\end{cases}$.\n",
    "    * Word Property: $p(prop\\mid word) = \\sum_{topic}p(prop\\mid topic)p(topic\\mid word)p(word) = \\sum_{topic}p(prop\\mid topic)p(word\\mid topic)p(topic)$.\n",
    "    * New Word Property I: Paradigmatic Cohort Technique: \n",
    "        * Find $new\\_word$'s predicate,\n",
    "        * Find the set of $new\\_word$'s paradigmatic cohort wrt. the predict,\n",
    "        * The updated property is the intersection of all the properties of the words in the cohort.\n",
    "    * New Word Property II: Syntagmatic Cohort Technique (TODO):\n",
    "    * Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"/Users/jacobsw/Desktop/CODER/IMPLEMENTATION_CAMP/BASIC_TOPICS/DISTRIBUTIONAL_SEMANTICS/DATA/McRae-BRM-InPress/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path+\"CONCS_FEATS_concstats_brm.xls\", delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'Concept', u'Feature', u'WB_Label', u'WB_Maj', u'WB_Min', u'BR_Label',\n",
       "       u'Prod_Freq', u'Rank_PF', u'Sum_PF_No_Tax', u'CPF', u'Disting',\n",
       "       u'Distinct', u'CV_No_Tax', u'Intercorr_Str_Tax',\n",
       "       u'Intercorr_Str_No_Tax', u'Feat_Length_Including_Spaces', u'Phon_1st',\n",
       "       u'KF', u'ln(KF)', u'BNC', u'ln(BNC)', u'Familiarity', u'Length_Letters',\n",
       "       u'Length_Phonemes', u'Length_Syllables', u'Bigram', u'Trigram',\n",
       "       u'ColtheartN', u'Num_Feats_Tax', u'Num_Feats_No_Tax',\n",
       "       u'Num_Disting_Feats_No_Tax', u'Disting_Feats_%_No_Tax',\n",
       "       u'Mean_Distinct_No_Tax', u'Mean_CV_No_Tax', u'Density_No_Tax',\n",
       "       u'Num_Corred_Pairs_No_Tax', u'%_Corred_Pairs_No_Tax', u'Num_Func',\n",
       "       u'Num_Vis_Mot', u'Num_VisF&S', u'Num_Vis_Col', u'Num_Sound',\n",
       "       u'Num_Taste', u'Num_Smell', u'Num_Tact', u'Num_Ency', u'Num_Tax'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Concept</th>\n",
       "      <th>Feature</th>\n",
       "      <th>WB_Label</th>\n",
       "      <th>WB_Maj</th>\n",
       "      <th>WB_Min</th>\n",
       "      <th>BR_Label</th>\n",
       "      <th>Prod_Freq</th>\n",
       "      <th>Rank_PF</th>\n",
       "      <th>Sum_PF_No_Tax</th>\n",
       "      <th>CPF</th>\n",
       "      <th>...</th>\n",
       "      <th>Num_Func</th>\n",
       "      <th>Num_Vis_Mot</th>\n",
       "      <th>Num_VisF&amp;S</th>\n",
       "      <th>Num_Vis_Col</th>\n",
       "      <th>Num_Sound</th>\n",
       "      <th>Num_Taste</th>\n",
       "      <th>Num_Smell</th>\n",
       "      <th>Num_Tact</th>\n",
       "      <th>Num_Ency</th>\n",
       "      <th>Num_Tax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accordion</td>\n",
       "      <td>a_musical_instrument</td>\n",
       "      <td>superordinate</td>\n",
       "      <td>c</td>\n",
       "      <td>h</td>\n",
       "      <td>taxonomic</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>accordion</td>\n",
       "      <td>associated_with_polkas</td>\n",
       "      <td>associated_entity</td>\n",
       "      <td>s</td>\n",
       "      <td>e</td>\n",
       "      <td>encyclopaedic</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>accordion</td>\n",
       "      <td>has_buttons</td>\n",
       "      <td>external_component</td>\n",
       "      <td>e</td>\n",
       "      <td>ce</td>\n",
       "      <td>visual-form_and_surface</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>163.0</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>accordion</td>\n",
       "      <td>has_keys</td>\n",
       "      <td>external_component</td>\n",
       "      <td>e</td>\n",
       "      <td>ce</td>\n",
       "      <td>visual-form_and_surface</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>108.0</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>accordion</td>\n",
       "      <td>inbeh_-_produces_music</td>\n",
       "      <td>entity_behavior</td>\n",
       "      <td>e</td>\n",
       "      <td>b</td>\n",
       "      <td>sound</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>178.0</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Concept                 Feature            WB_Label WB_Maj WB_Min  \\\n",
       "0  accordion    a_musical_instrument       superordinate      c      h   \n",
       "1  accordion  associated_with_polkas   associated_entity      s      e   \n",
       "2  accordion             has_buttons  external_component      e     ce   \n",
       "3  accordion                has_keys  external_component      e     ce   \n",
       "4  accordion  inbeh_-_produces_music     entity_behavior      e      b   \n",
       "\n",
       "                  BR_Label  Prod_Freq  Rank_PF  Sum_PF_No_Tax  CPF   ...     \\\n",
       "0                taxonomic         28        1            NaN   18   ...      \n",
       "1            encyclopaedic          9        4            9.0    1   ...      \n",
       "2  visual-form_and_surface          8        5          163.0   13   ...      \n",
       "3  visual-form_and_surface         17        2          108.0    7   ...      \n",
       "4                    sound          6        7          178.0   13   ...      \n",
       "\n",
       "  Num_Func  Num_Vis_Mot  Num_VisF&S  Num_Vis_Col  Num_Sound  Num_Taste  \\\n",
       "0        2            0           2            0          2          0   \n",
       "1        2            0           2            0          2          0   \n",
       "2        2            0           2            0          2          0   \n",
       "3        2            0           2            0          2          0   \n",
       "4        2            0           2            0          2          0   \n",
       "\n",
       "  Num_Smell  Num_Tact  Num_Ency  Num_Tax  \n",
       "0         0         0         2        1  \n",
       "1         0         0         2        1  \n",
       "2         0         0         2        1  \n",
       "3         0         0         2        1  \n",
       "4         0         0         2        1  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "norms = list(set(df['Concept']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Are all norms in Brown?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "from string import punctuation as punc\n",
    "from spacy.en import English, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "brown_words = list(brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "brown_fdist = Counter(brown_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "brown_sents = [unicode(' '.join(sent)) for sent in brown.sents()]\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_vocab_info(sents, parser):\n",
    "    \n",
    "    parsed_sents = [parser(sent) for sent in sents]\n",
    "    token_set = {token.orth_ for parsed_sent in parsed_sents for token in parsed_sent}\n",
    "    lemma_set = {token.lemma_ for parsed_sent in parsed_sents for token in parsed_sent}\n",
    "    lemma2pos = defaultdict(set)\n",
    "    token2pos = defaultdict(set)\n",
    "    token2lemma = {}\n",
    "    for parsed_sent in parsed_sents:\n",
    "        for token in parsed_sent:\n",
    "            lemma2pos[token.lemma_].add(token.pos_)\n",
    "            token2pos[token.orth_].add(token.pos_)\n",
    "            token2lemma[token.orth_] = token.lemma_\n",
    "    \n",
    "    return token_set, token2pos, lemma_set, lemma2pos, token2lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 41s, sys: 446 ms, total: 1min 41s\n",
      "Wall time: 1min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "brown_tokens, brown_t2p, brown_lemmas, brown_l2p, brown_t2l = get_vocab_info(brown_sents, parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Norm Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def norm_cleanup(norms, corpus_tokens=brown_tokens, corpus_lemmas=brown_lemmas, corpus_t2l=brown_t2l):\n",
    "    \n",
    "    for i,norm in enumerate(norms):\n",
    "        if '_' in norm: \n",
    "            norm = norm.split('_')[0]\n",
    "            norms[i] = norm\n",
    "        if norm in corpus_lemmas: pass\n",
    "        elif norm in corpus_tokens:\n",
    "            norms[i] = corpus_t2l[norm]\n",
    "        else: pass\n",
    "        \n",
    "    return norms\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = list(set(df['Feature'])) # i.e. properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_norm2feature_dict(df):\n",
    "    \n",
    "    norm2feature = defaultdict(int)\n",
    "    for i in xrange(len(df)):\n",
    "        norm = norm_cleanup([df.ix[i]['Concept']])[0]\n",
    "        feature = df.ix[i]['Feature']\n",
    "        norm2feature[(norm,feature)] += 1\n",
    "    \n",
    "    return norm2feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "norm2feature = make_norm2feature_dict(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Topic Modeling for Properties\n",
    "\n",
    "* $p(prop\\mid topic) = \\sum_{word\\in topic}p(word\\mid topic)f(word,prop)$, where $f(word,prop) = \\begin{cases}1 & \\text{if word has prop}\\\\ 0 & otherwise\\end{cases}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** a. Facilities (Gensim) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TopicModel:\n",
    "    \n",
    "    def __init__(self, documents, num_topics=3, num_iters=10):\n",
    "        dic = corpora.Dictionary(documents)\n",
    "        documents = [dic.doc2bow(document) for document in documents]\n",
    "        self.documents = documents\n",
    "        self.lda = LdaModel(documents, num_topics=num_topics, id2word=dic, passes=num_iters)\n",
    "    \n",
    "    def summarize(self, num_topics=3, num_words=5):\n",
    "        freq_stats = self.lda.print_topics(num_topics=num_topics, num_words=num_words)\n",
    "        print \"Statistics\"\n",
    "        print \n",
    "        for i,document in enumerate(self.documents):\n",
    "            print \"Most %d-Frequent Words in Topic %d:\" % (num_words,freq_stats[i][0])\n",
    "            print freq_stats[i][1]\n",
    "            print \"Topic-Distribution:\"\n",
    "            print self.lda.get_document_topics(document)\n",
    "            print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** b. Make DL10 Fake Documents **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "brown_sents = list(brown.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokensent2lemmasent(sents, fdist, norms, freq=20):\n",
    "    \n",
    "    parsed_sents = [parser(unicode(' '.join(sent))) for sent in sents]\n",
    "    sents_in_lemmas = []\n",
    "    for parsed_sent in parsed_sents:\n",
    "        sent = []\n",
    "        for token in parsed_sent:\n",
    "            if token.orth_ in norms or token.lemma_ in norms:\n",
    "                sent.append(token.lemma_)\n",
    "            elif fdist[token.orth_] < freq \\\n",
    "                or token.lemma_ in STOPWORDS \\\n",
    "                or token.lemma_ in punc: continue\n",
    "            else: \n",
    "                sent.append(token.lemma_)\n",
    "        sents_in_lemmas.append(sent)\n",
    "    \n",
    "    return sents_in_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_norm_fdist(norms, fdist, t2l):\n",
    "    \n",
    "    ret = []\n",
    "    for norm in norms:\n",
    "        if norm in fdist:\n",
    "            ret.append((norm,fdist[norm]))\n",
    "        elif norm in t2l:\n",
    "            norm = t2l[norm]\n",
    "            if norm in fdist: \n",
    "                ret.append((norm,fdist[norm]))\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm_fdist = get_norm_fdist(norms, brown_fdist, brown_t2l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "norms_set = set(norms) # for quickening 'in norms' check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngfx/MevBGDKdHs4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19cfbd090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(sorted([count for norm,count in norm_fdist], reverse=True))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 20s, sys: 198 ms, total: 1min 21s\n",
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "brown_sents = tokensent2lemmasent(brown_sents, brown_fdist, norms_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = {word for sent in brown_sents for word in sent}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3992"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([norm for norm in norms if norm not in vocab]) # out of vocab norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_cooccurrence_matrix(sents, vocab, win_size):\n",
    "    \n",
    "    w2i = {w:i for i,w in enumerate(vocab)}\n",
    "    print \"... building dictionary\"\n",
    "    cooccurrence_dict = defaultdict(int)\n",
    "    for sent in sents:\n",
    "        for i,target in enumerate(sent):\n",
    "            contexts = sent[max(0,i-win_size):i] + sent[min(i+1,len(sent)):min(i+1+win_size,len(sent))]\n",
    "            for context in contexts:\n",
    "                cooccurrence_dict[(target,context)] += 1\n",
    "    print \"... building cooccurrence matrix\"\n",
    "    cooccurrence_matrix = np.zeros((len(vocab),len(vocab)))\n",
    "    for target in vocab:\n",
    "        for context in vocab:\n",
    "            cooccurrence_matrix[w2i[target]][w2i[context]] += cooccurrence_dict[(target,context)]\n",
    "    \n",
    "    return w2i, cooccurrence_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... building dictionary\n",
      "... building cooccurrence matrix\n",
      "CPU times: user 22.7 s, sys: 1.85 s, total: 24.6 s\n",
      "Wall time: 24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "w2i, cooccurrence_matrix = build_cooccurrence_matrix(brown_sents, vocab, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# path = \"/Users/jacobsw/Desktop/UNIV/FALL_2016/LIN389C_RSCH_COMPLING/BAYESIAN/CODE_DRAFTS/DATA/\"\n",
    "# cPickle.dump(cooccurrence_matrix, open(path+\"coccurrence_matrix.p\",'wb'))\n",
    "# coocurrence_matrix = cPickle.load(open(path+\"coccurrence_matrix.p\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def make_fake_documents(matrix, vocab):\n",
    "    \n",
    "#     docs = []\n",
    "#     for i,row in enumerate(matrix):\n",
    "#         if i!=0 and i%1000==0: print \"... processed %d targets.\" % i\n",
    "#         doc = []\n",
    "#         for j,cell in enumerate(row): # cell (i,j) count.\n",
    "#             doc += [vocab[j]] * cell\n",
    "#         docs.append(doc)\n",
    "    \n",
    "#     return docs\n",
    "\n",
    "def make_fake_documents(matrix, vocab):\n",
    "    \n",
    "    docs = []\n",
    "    m, n = matrix.shape\n",
    "    for i in xrange(m):\n",
    "        if i!=0 and i%1000==0: print \"... processed %d targets.\" % i\n",
    "        doc = []\n",
    "        for j in xrange(n): \n",
    "            doc += [vocab[j]] * matrix[i][j]\n",
    "        docs.append(doc)\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i2w = {i:w for w,i in w2i.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... processed 1000 targets.\n",
      "... processed 2000 targets.\n",
      "... processed 3000 targets.\n",
      "CPU times: user 30.6 s, sys: 1.49 s, total: 32.1 s\n",
      "Wall time: 31.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "docs = make_fake_documents(cooccurrence_matrix, i2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cPickle.dump(docs, open(path+\"fake_documents.p\",'wb'))\n",
    "# docs = cPickle.load(open(path+\"fake_documents.p\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "limited\n",
      "[u'child', u'risk', u'govern', u'force', u'study', u'study', u'credit', u'total', u'involve', u'involve', u'want', u'thing', u'provide', u'provide', u'provide', u'parallel', u'devote', u'production', u'free', u'free', u'playing', u'struggle', u'fiction', u'project', u'matter', u'manner', u'shall', u'earth', u'adequate', u'despite', u'future', u'future', u'extent', u'protection', u'skill', u'life', u'procedure', u'suggest', u'social', u'possible', u'possible', u'old', u'successfully', u'support', u'way', u'way', u'war', u'war', u\"''\", u\"''\", u\"''\", u'generally', u'realize', u'painting', u'daily', u'time', u'time', u'people', u'people', u'people', u'choice', u'discover', u'assistance', u'assistance', u'assistance', u'extra', u'merely', u'today', u'``', u'``', u'``', u'effort', u'discussion', u'vital', u'man', u'man', u'explicit', u'nation', u'half', u'2', u'2', u'white', u'population', u'ground', u'slow', u'resource', u'artist', u'relative', u'federal', u'period', u'direction', u'shopping', u'evidence', u'evidence', u'application', u'central', u'mean', u'area', u'area', u'regard', u'certain', u'field', u'wife', u'external', u'far', u'--', u'--', u'--', u'--', u'--', u'--', u'large', u'past', u'christian', u'family', u'company', u'sphere', u'sense', u'sense', u'information', u'soon', u'good', u'operation', u'event', u'issue', u'circumstance', u'base', u'major', u'major', u'number', u'number', u'number', u'number', u'number', u'number', u'station', u'reflection', u'kind', u'instruction', u'essentially', u'tool', u'plan', u'appear', u'cover', u'particularly', u'phenomenon', u'appeal', u'activity', u'arm', u'available', u'objective', u'create', u'political', u'laos', u'solid', u'development', u'recent', u'task', u'atomic', u'continue', u'essential', u'economic', u'use', u'use', u'abroad', u'range', u'question', u'access', u'influence', u'age', u'power', u'power', u'exercise', u'great', u'hardy', u'apply', u'stage', u'impress', u'obvious', u'effectively', u'legislative', u'need', u'need', u'face', u'knowledge', u'knowledge', u'hope', u'insight', u'nuclear', u'job', u'approval', u'discrimination', u'define', u'general', u'resident', u'member', u'position', u'graduate', u'government', u'government', u'success', u'success', u'authority', u'authority', u'growth', u'basis', u'intention', u'problem', u'class']\n"
     ]
    }
   ],
   "source": [
    "print i2w[1]\n",
    "print docs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** c. Topic Modeling on Fake Docs **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 7s, sys: 2.06 s, total: 2min 9s\n",
      "Wall time: 2min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tm = TopicModel(docs, num_topics=100, num_iters=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cPickle.dump(tm, open(path+\"topic_model.p\",'wb'))\n",
    "# tm = cPickle.load(open(path+\"topic_model.p\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(20, 0.3883246604863429), (49, 0.53629072412904299)]"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm.lda.get_document_topics(tm.documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(98, 0.013330662998430783),\n",
       " (3678, 0.012625443555063431),\n",
       " (191, 0.011901291855010704),\n",
       " (1093, 0.011480955669982853),\n",
       " (228, 0.010758324296336618),\n",
       " (533, 0.0088845763781074394),\n",
       " (138, 0.008465844931493223),\n",
       " (439, 0.008383028928491295),\n",
       " (276, 0.0078938102793168884),\n",
       " (412, 0.007718434693532613)]"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm.lda.get_topic_terms(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'watch'"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm.lda.id2word[645]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tm_i2w = {i:w for i,w in tm.lda.id2word.iteritems()}\n",
    "tm_w2i = {w:i for i,w in tm.lda.id2word.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_topic_word_dict(lda, ntopics=100):\n",
    "    \n",
    "    nwords = lda.num_terms\n",
    "    i2w = lda.id2word\n",
    "    topicword_dict = defaultdict(float)\n",
    "    for i in xrange(ntopics):\n",
    "        for w_id,prob in lda.get_topic_terms(i,topn=nwords):\n",
    "            topicword_dict[(i,i2w[w_id])] = prob\n",
    "    \n",
    "    return topicword_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 717 ms, sys: 39.8 ms, total: 757 ms\n",
      "Wall time: 751 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "topicword_dict = make_topic_word_dict(tm.lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.867030818011924e-06"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topicword_dict[(0,'cat')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def p_word_given_topic(word, topic):\n",
    "    return topicword_dict[(topic,word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f_word_prop(word, prop):\n",
    "    return norm2feature[(word,prop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.86703081801e-06\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print p_word_given_topic('cat',0)\n",
    "print f_word_prop('accordion','a_musical_instrument')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_topic_prop_dict(lda, ntopics=100):\n",
    "    \n",
    "    nwords = lda.num_terms\n",
    "    i2w = lda.id2word\n",
    "    topicprop_dict = defaultdict(float)\n",
    "    for i in xrange(ntopics):\n",
    "        if i!=0 and i%5==0: print \"... processed %d topics.\" % i\n",
    "        for prop in features: # features defined at the top.\n",
    "            ret = 0.0\n",
    "            for w_id,prob in lda.get_topic_terms(i,topn=nwords):\n",
    "                word = i2w[w_id]\n",
    "                ret += p_word_given_topic(word,i)*f_word_prop(word,prop)\n",
    "            topicprop_dict[(i,prop)] = ret\n",
    "    \n",
    "    return topicprop_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... processed 5 topics.\n",
      "... processed 10 topics.\n",
      "... processed 15 topics.\n",
      "... processed 20 topics.\n",
      "... processed 25 topics.\n",
      "... processed 30 topics.\n",
      "... processed 35 topics.\n",
      "... processed 40 topics.\n",
      "... processed 45 topics.\n",
      "... processed 50 topics.\n",
      "... processed 55 topics.\n",
      "... processed 60 topics.\n",
      "... processed 65 topics.\n",
      "... processed 70 topics.\n",
      "... processed 75 topics.\n",
      "... processed 80 topics.\n",
      "... processed 85 topics.\n",
      "... processed 90 topics.\n",
      "... processed 95 topics.\n",
      "CPU times: user 45min 39s, sys: 6.88 s, total: 45min 46s\n",
      "Wall time: 45min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "topicprop_dict = make_topic_prop_dict(tm.lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cPickle.dump(topicprop_dict, open(path+\"topicprop_dict.p\",'wb'))\n",
    "# topicprop_dict = cPickle.load(open(path+\"topicprop_dict.p\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def p_prop_given_topic(prop, topic):\n",
    "    return topicprop_dict[(topic,prop)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Word Properties\n",
    "\n",
    "* $p(prop\\mid word) = \\sum_{topic}p(prop\\mid topic)p(topic\\mid word)p(word) = \\sum_{topic}p(prop\\mid topic)p(word\\mid topic)p(topic)$.\n",
    "* NB: $p(prop\\mid topic)p(topic\\mid word)p(word) = \\frac{p(word\\mid topic)p(topic)}{p(word)}p(word) = p(word\\mid topic)p(topic)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = [word for doc in docs for word in doc]\n",
    "nwords = len(words)\n",
    "word_counts = Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def make_wordprop_dict(ntopics=100):\n",
    "    \n",
    "#     p_topic = 1/ntopics\n",
    "#     wordprop_dict = defaultdict(float)\n",
    "#     for n,word in enumerate(words):\n",
    "#         if n!=0 and n%500==0: \n",
    "#             print \"... processed %d words.\" % n\n",
    "#             break\n",
    "#         for prop in features:\n",
    "#             ret = 0.0\n",
    "#             for i in xrange(ntopics):\n",
    "#                 ret += p_prop_given_topic(prop,i)*p_word_given_topic(word,i)*p_topic\n",
    "#             wordprop_dict[(word,prop)] = ret\n",
    "    \n",
    "#     return wordprop_dict\n",
    "\n",
    "# %%time\n",
    "# wordprop_dict = make_wordprop_dict()\n",
    "# ... processed 500 words.\n",
    "# CPU times: user 1min 43s, sys: 135 ms, total: 1min 43s\n",
    "# Wall time: 1min 43s\n",
    "\n",
    "# ~ 70.8 hrs to run all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def p_prop_given_word(prop, word, ntopics=100):\n",
    "    \n",
    "    p_topic = 1/ntopics\n",
    "    ret = 0.0\n",
    "    for i in xrange(ntopics):\n",
    "        ret += p_prop_given_topic(prop,i)*p_word_given_topic(word,i)*p_topic\n",
    "        \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_k_prop(word, k=10, verbose=1):\n",
    "    \n",
    "    props = [(i,p_prop_given_word(prop,word)) for i,prop in enumerate(features)]\n",
    "    top_k = nlargest(k, props, key=lambda x:x[1])\n",
    "    if verbose:\n",
    "        for i,(idx,prob) in enumerate(top_k):\n",
    "            print \"%dth property: %s (prob=%.6f%%).\" % (i+1,features[idx],prob*100)\n",
    "    \n",
    "    top_k_props = map(lambda (idx,prob): features[idx], top_k)\n",
    "    \n",
    "    return set(top_k_props)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th property: is_large (prob=0.001054%).\n",
      "2th property: made_of_metal (prob=0.001035%).\n",
      "3th property: made_of_wood (prob=0.000965%).\n",
      "4th property: is_long (prob=0.000698%).\n",
      "5th property: found_in_schools (prob=0.000657%).\n",
      "6th property: used_for_learning (prob=0.000657%).\n",
      "7th property: is_small (prob=0.000589%).\n",
      "8th property: made_of_plastic (prob=0.000570%).\n",
      "9th property: made_of_paper (prob=0.000557%).\n",
      "10th property: has_pictures (prob=0.000527%).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'found_in_schools',\n",
       " 'has_pictures',\n",
       " 'is_large',\n",
       " 'is_long',\n",
       " 'is_small',\n",
       " 'made_of_metal',\n",
       " 'made_of_paper',\n",
       " 'made_of_plastic',\n",
       " 'made_of_wood',\n",
       " 'used_for_learning'}"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_prop('book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th property: made_of_wood (prob=0.000050%).\n",
      "2th property: is_large (prob=0.000044%).\n",
      "3th property: is_small (prob=0.000044%).\n",
      "4th property: made_of_metal (prob=0.000043%).\n",
      "5th property: an_animal (prob=0.000040%).\n",
      "6th property: is_round (prob=0.000029%).\n",
      "7th property: has_4_legs (prob=0.000027%).\n",
      "8th property: is_long (prob=0.000025%).\n",
      "9th property: is_edible (prob=0.000025%).\n",
      "10th property: has_legs (prob=0.000024%).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'an_animal',\n",
       " 'has_4_legs',\n",
       " 'has_legs',\n",
       " 'is_edible',\n",
       " 'is_large',\n",
       " 'is_long',\n",
       " 'is_round',\n",
       " 'is_small',\n",
       " 'made_of_metal',\n",
       " 'made_of_wood'}"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_prop('alligator')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. New Word Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* a. ** New Word Property I: Paradigmatic Cohort Technique **: \n",
    "    * Find $new\\_word$'s predicate,\n",
    "    * Find the set of $new\\_word$'s paradigmatic cohort wrt. the predict,\n",
    "    * The updated property is the intersection of all the properties of the words in the cohort.\n",
    "\n",
    "**NB**: May not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( i | nsubj | eat )\n",
      "( eat | ROOT | eat )\n",
      "( a | det | cat )\n",
      "( cat | dobj | eat )\n",
      "( that | nsubj | attack )\n",
      "( attack | relcl | cat )\n",
      "( cat | dobj | attack )\n",
      "( . | punct | eat )\n"
     ]
    }
   ],
   "source": [
    "t = u'i ate a cat that attacks cats.'\n",
    "parsed_t = parser(t)\n",
    "for token in parsed_t:\n",
    "    print '(', token.lemma_, '|', token.dep_, '|', token.head.lemma_, ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_prop(word, k=10):\n",
    "    \n",
    "    props = [(i,p_prop_given_word(prop,word)) for i,prop in enumerate(features)]\n",
    "    top_k = nlargest(k, props, key=lambda x:x[1])\n",
    "    \n",
    "    return {features[idx] for idx,prop in top_k}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['found_in_schools', 'made_of_paper', 'made_of_metal', 'is_long', 'made_of_plastic', 'used_for_learning', 'is_small', 'has_pictures', 'is_large', 'made_of_wood'])\n"
     ]
    }
   ],
   "source": [
    "print get_prop('book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pred_args(parsed_sent):\n",
    "    \n",
    "    pred_args = defaultdict(lambda : defaultdict(set))\n",
    "    for token in parsed_sent: \n",
    "        if token.dep_.endswith('subj'):\n",
    "            pred_args[token.head.lemma_]['S'].add(token.lemma_)\n",
    "        elif token.dep_.endswith('obj'):\n",
    "            pred_args[token.head.lemma_]['O'].add(token.lemma_)\n",
    "    \n",
    "    return pred_args\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>>,\n",
       "            {u'attack': defaultdict(set, {'O': {u'cat'}, 'S': {u'that'}}),\n",
       "             u'eat': defaultdict(set, {'O': {u'cat'}, 'S': {u'i'}})})"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pred_args(parsed_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 28s, sys: 170 ms, total: 1min 28s\n",
      "Wall time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sents = [unicode(' '.join(sent)) for sent in brown.sents()]\n",
    "parsed_sents = [parser(sent) for sent in sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_dep_triples(parsed_sents):\n",
    "    \n",
    "    triples = []\n",
    "    for parsed_sent in parsed_sents:\n",
    "        for token in parsed_sent:\n",
    "            token_triple = (token.orth_, token.dep_, token.head.orth_) # may be useful later.\n",
    "            lemma_triple = (token.lemma_, token.dep_, token.head.lemma_)\n",
    "            triples.append([token_triple, lemma_triple])\n",
    "    \n",
    "    return triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.01 s, sys: 465 ms, total: 7.48 s\n",
      "Wall time: 7.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dep_triples = extract_dep_triples(parsed_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_para_cohort_dict(triples): \n",
    "    \n",
    "    para_dict = defaultdict(lambda : defaultdict(set))\n",
    "    for token_triples,lemma_triples in triples:\n",
    "        if token_triples[1].endswith('subj'):\n",
    "            para_dict[lemma_triples[2]]['S'].add(lemma_triples[0])\n",
    "        elif token_triples[1].endswith('obj'):\n",
    "            para_dict[lemma_triples[2]]['O'].add(lemma_triples[0])\n",
    "    \n",
    "    return para_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.15 s, sys: 32.2 ms, total: 1.18 s\n",
      "Wall time: 1.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "para_dict = make_para_cohort_dict(dep_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<type 'set'>, {'S': set([u'constitution']), 'O': set([u'message', u'much', u'them', u'child'])})\n",
      "defaultdict(<type 'set'>, {'S': set([u'student', u'people', u'fish', u'it', u'one', u'americans', u'which', u'lord', u'you', u'we', u'animal', u'parent', u'who', u'dave', u'they', u'child', u'gasp', u'he', u'boy', u'i', u'charlie', u'she']), 'O': set([u'tamale', u'garbage', u'flesh', u'it', u'dust', u'seed', u'skin', u'breakfast', u'chicken', u'what', u'profit', u'facility', u'dinner', u'celery', u'other', u'enough', u'much', u'bread', u'hat', u'rider', u'more', u'body', u'leg', u'food', u'luncheon', u'fat', u'cereal', u'supper', u'nothing', u'man', u'stress', u'piece', u'salad', u'this', u'amount', u'soup', u'portion', u'daisy', u'mussel', u'litle', u'egg', u'meal'])})\n"
     ]
    }
   ],
   "source": [
    "print para_dict['copy']\n",
    "print para_dict['eat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nonzero_prop(word):\n",
    "    \n",
    "    props = [(i,p_prop_given_word(prop,word)) for i,prop in enumerate(features)]  \n",
    "    nonzeros = map(lambda (idx,prob):features[idx], props)\n",
    "    \n",
    "    return set(nonzeros)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def infer_para_prop(word, pred, rel, k=300):\n",
    "    \n",
    "    para_cohort = list(para_dict[pred][rel])\n",
    "    prop_list = map(lambda word: top_k_prop(word,k,verbose=0), para_cohort)\n",
    "    prop_intersection = set.intersection(*prop_list)\n",
    "    \n",
    "    return prop_intersection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l = infer_para_prop('cat','eat','S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a_bird',\n",
       " 'a_carnivore',\n",
       " 'has_a_beak',\n",
       " 'is_loud',\n",
       " 'is_red',\n",
       " 'lives_in_wilderness',\n",
       " 'made_of_cement',\n",
       " 'made_of_glass',\n",
       " 'tastes_good',\n",
       " 'used_for_transportation',\n",
       " 'worn_by_women'}"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
