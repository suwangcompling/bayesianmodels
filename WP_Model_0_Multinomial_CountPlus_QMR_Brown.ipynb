{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Count (Syntax-Context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0a. Coding Info\n",
    "\n",
    "* **Corpus**\n",
    "    * Brown\n",
    "    * McRae Norms\n",
    "* **Parser**\n",
    "    * SpaCy\n",
    "* **Algorithm**\n",
    "    * Count-based Multinomial Bayesian Updating (Erk 2016)\n",
    "* **Evaluation**\n",
    "    * Precision @1/@5\n",
    "    * Mean Average Precision (MAP) https://www.youtube.com/watch?v=pM6DJ0ZZee0&index=12&list=PLBv09BD7ez_6nqE9YU9bQXpjJ5jJ1Kgr9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0b. Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "from __future__ import division\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0c. Input Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "concepts,cpt2ft,cpt2ftprob,features = cPickle.load(open(\"/Users/jacobsw/Desktop/UNIV/FALL_2016/LIN389C_RSCH_COMPLING/BAYESIAN/DATA/MCRAE/qmr.p\",'rb'))\n",
    "triples = cPickle.load(open(\"/Users/jacobsw/Desktop/UNIV/FALL_2016/LIN389C_RSCH_COMPLING/BAYESIAN/DATA/BROWN/brown_triples.p\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(concepts)==len(set(concepts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize ftprobs\n",
    "for cpt in concepts:\n",
    "    cpt2ftprob[cpt] = list(np.array(cpt2ftprob[cpt]) / np.array(cpt2ftprob[cpt]).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f2i = {f:i for i,f in enumerate(features)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cpt2ftprob_dic = defaultdict(dict)\n",
    "for cpt in concepts:\n",
    "    cpt2ftprob_dic[cpt] = {ft:ftprob for ft,ftprob in zip(cpt2ft[cpt],cpt2ftprob[cpt])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'found_in_libraries': 0.030837004405286337,\n",
       " 'found_in_schools': 0.030837004405286337,\n",
       " 'found_on_shelves': 0.083700440528634346,\n",
       " 'has_a_hard_cover': 0.030837004405286337,\n",
       " 'has_a_soft_cover': 0.030837004405286337,\n",
       " 'has_authors': 0.088105726872246687,\n",
       " 'has_information': 0.088105726872246687,\n",
       " 'has_page_numbers': 0.088105726872246687,\n",
       " 'has_pages': 0.088105726872246687,\n",
       " 'has_pictures': 0.030837004405286337,\n",
       " 'has_words_in_it': 0.088105726872246687,\n",
       " 'made_of_paper': 0.083700440528634346,\n",
       " 'tells_stories': 0.030837004405286337,\n",
       " 'used_by_reading': 0.088105726872246687,\n",
       " 'used_for_acquiring/storing_knowledge': 0.088105726872246687,\n",
       " 'used_for_learning': 0.030837004405286337}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpt2ftprob_dic['book']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0f. General Purpose Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(arr):\n",
    "    return arr / arr.sum()\n",
    "\n",
    "def partition(l, k):\n",
    "    \"\"\" args: list, # partitions.\n",
    "        returns: a list of k partitions.\n",
    "    \"\"\"\n",
    "    k = max(1, k)\n",
    "    chunk_size = len(l)//k\n",
    "    return [l[i:i+chunk_size] for i in xrange(0, len(l), chunk_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Mappings\n",
    "\n",
    "* Verb-Role $\\mapsto$ Concept List/Set Mapping\n",
    "* Concept $\\mapsto$ Verb-Role List/Set Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preproc_mapping(concepts, triples):\n",
    "    \"\"\" args: concepts, features, concept->feature mapping, concept->P(feature) mapping, dependency triples.\n",
    "        returns: verb-role->concept-set mapping.\n",
    "    \"\"\"\n",
    "    concepts_set = set(concepts) # for fast lookup.\n",
    "    vr2c = defaultdict(set)\n",
    "    c2vr = defaultdict(list)\n",
    "    for word,dep,head in triples:\n",
    "        if word in concepts_set and (dep=='nsubj' or dep=='dobj' or dep=='amod'):\n",
    "            vr2c[head+'-'+dep].add(word)\n",
    "            c2vr[word].append(head+'-'+dep)\n",
    "    return vr2c, c2vr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vr2c, c2vr = preproc_mapping(concepts, triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1110\n"
     ]
    }
   ],
   "source": [
    "vrs = vr2c.keys() # global verb-role pair indexing.\n",
    "vr2i = {vr:i for i,vr in enumerate(vrs)}\n",
    "print len(vrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319\n"
     ]
    }
   ],
   "source": [
    "concepts_subset = c2vr.keys() # ORDER DOESN'T MATTER\n",
    "print len(concepts_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preproc_split(concepts, k=5):\n",
    "    \"\"\" arg: concepts, cv numbers.\n",
    "        returns: k .8/.2 train-test splits.\n",
    "    \"\"\"\n",
    "    # random.shuffle(concepts) # TODO: messing up with the indexing. will fix later.\n",
    "    concept_chunks = partition(concepts, k)\n",
    "    train_test_splits = []\n",
    "    for i in range(k):\n",
    "        train_cpts = list(chain.from_iterable([concept_chunk for j,concept_chunk in enumerate(concept_chunks)\n",
    "                                               if j!=i]))\n",
    "        test_cpts = concept_chunks[i]\n",
    "        train_test_splits.append((train_cpts,test_cpts))\n",
    "    return train_test_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_test_splits = preproc_split(concepts_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'cape', u'banner', u'bread', u'wagon', u'bat', u'peg', u'peacock', u'tongs', u'level', u'cigar', u'bracelet', u'gun', u'pencil', u'swimsuit', u'axe', u'bag', u'microscope', u'hatchet', u'gloves', u'subway', u'rock', u'nightingale', u'mouse', u'garage', u'tray', u'hawk', u'lantern', u'turtle', u'butterfly', u'shed', u'fence', u'cage', u'house', u'duck', u'crown', u'pepper', u'worm', u'lion', u'cellar', u'owl', u'mirror', u'candle', u'hammer', u'chicken', u'whip', u'barn', u'moth', u'closet', u'clock', u'kettle', u'fox', u'revolver', u'vest', u'mug', u'stool', u'bureau', u'pistol', u'slingshot', u'knife', u'tent', u'cockroach', u'sheep', u'cantaloupe', u'hose', u'umbrella', u'camel', u'pear', u'corn', u'cart', u'cork', u'guitar', u'spoon', u'grasshopper', u'sink', u'key', u'goldfish', u'comb', u'grenade', u'card', u'box', u'stone', u'drum', u'jet', u'cow', u'saddle', u'jeep', u'walrus', u'biscuit', u'shield', u'willow', u'anchor', u'emerald', u'magazine', u'pillow', u'trailer', u'falcon', u'ox', u'coin', u'buckle', u'scarf', u'clamp', u'shovel', u'apple', u'spear', u'toilet', u'wall', u'jar', u'pot', u'bayonet', u'coconut', u'sack', u'drain', u'vine', u'brush', u'church', u'table', u'ring', u'boat', u'belt', u'turkey', u'razor', u'horse', u'toy', u'van', u'certificate', u'pearl', u'medal', u'sweater', u'doll', u'frog', u'submarine', u'crow', u'helicopter', u'brick', u'limousine', u'banana', u'calf', u'fork', u'tomato', u'shell', u'door', u'squirrel', u'bus', u'shawl', u'buggy', u'envelope', u'faucet', u'tripod', u'missile', u'hut', u'train', u'stick', u'rabbit', u'rifle', u'olive', u'coyote', u'crane', u'rooster', u'radish', u'robe', u'tortoise', u'tank', u'car', u'ruler', u'cap', u'chisel', u'rat', u'bed', u'cat', u'rope', u'cabin', u'drill', u'coat', u'cake', u'bathtub', u'plug', u'tractor', u'shrimp', u'bridge', u'donkey', u'mink', u'tap', u'pin', u'canoe', u'pie', u'seal', u'deer', u'telephone', u'pig', u'ant', u'inn', u'carrot', u'cathedral', u'violin', u'ship', u'bow', u'skirt', u'airplane', u'spade', u'lamb', u'jacket', u'lemon', u'shirt', u'peach', u'mole', u'dress', u'balloon', u'bowl', u'cloak', u'couch', u'python', u'lamp', u'book', u'parakeet', u'sword', u'elephant', u'tie', u'saucer', u'goat', u'pan', u'typewriter', u'wheel', u'ball', u'cupboard', u'mat', u'snail', u'toad', u'football', u'pineapple', u'bucket', u'cigarette', u'tack', u'garlic', u'salamander', u'apron', u'cabbage', u'cabinet', u'veil', u'building', u'stereo', u'cod', u'bullet', u'potato', u'shotgun', u'urn', u'napkin', u'dog', u'sardine', u'pipe', u'truck', u'bottle', u'ambulance', u'basket', u'barrel', u'buffalo', u'eagle']\n",
      "\n",
      "[u'taxi', u'gown', u'bull', u'bomb', u'kite', u'rocket', u'sparrow', u'pigeon', u'stove', u'desk', u'doorknob', u'bin', u'pony', u'cottage', u'rattle', u'radio', u'oven', u'bedroom', u'dish', u'pheasant', u'chair', u'dove', u'rattlesnake', u'carpet', u'muzzle', u'apartment', u'grape', u'chain', u'cup', u'basement', u'menu', u'broom', u'swan', u'avocado', u'pen', u'bench', u'celery', u'bouquet', u'thimble', u'tape', u'board', u'wasp', u'orange', u'piano', u'rice', u'pumpkin', u'sailboat', u'plate', u'crocodile', u'lime', u'marble', u'cannon', u'thermometer', u'vulture', u'bear', u'baton', u'whistle', u'caterpillar', u'clam', u'fan', u'cushion', u'gate', u'flea']\n"
     ]
    }
   ],
   "source": [
    "print train_test_splits[0][0]; print\n",
    "print train_test_splits[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** ================== NB: The objects above this line are globally referenced!! ==================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_ft_vec(cpt):\n",
    "    \"\"\" args: a concept\n",
    "        returns: a len(features) array of feature probs in numpy array.\n",
    "    \"\"\"\n",
    "    return np.array([cpt2ftprob_dic[cpt][ft] if ft in cpt2ftprob_dic[cpt] else 0 for ft in features])\n",
    "\n",
    "def learn(train_cpts):\n",
    "    \"\"\" args: training concept set,\n",
    "        returns: verb-role->len(features) feature weights vector in numpy array.\n",
    "    \"\"\"\n",
    "    vr2ftdist = defaultdict(lambda : np.ones(len(features)))\n",
    "    for vr in vrs:\n",
    "        cpts = list(vr2c[vr])\n",
    "        for cpt in filter(lambda cpt:cpt in train_cpts, cpts):\n",
    "            vr2ftdist[vr] += to_ft_vec(cpt)\n",
    "    return vr2ftdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def infer(train_test_splits):\n",
    "    \"\"\" args: a list of train-test concept-list tuples.\n",
    "        returns: concept->len(features) feature weights vector in numpy array.\n",
    "    \"\"\"\n",
    "    cpt2ftdist = defaultdict(lambda : np.ones(len(features)))\n",
    "    for train_cpts,test_cpts in train_test_splits:\n",
    "        vr2ftdist = learn(train_cpts)\n",
    "        for cpt in test_cpts:\n",
    "            ftdist = np.ones(len(features))\n",
    "            for vr in c2vr[cpt]:\n",
    "                vr_ftdist = vr2ftdist[vr]\n",
    "                for f_i in range(len(features)):\n",
    "                    if vr_ftdist[f_i]>1: # means there's evidence at this feature.\n",
    "                        ftdist[f_i] += vr_ftdist[f_i]\n",
    "            cpt2ftdist[cpt] = ftdist # feature weights vector.\n",
    "    return cpt2ftdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.51 s, sys: 50.6 ms, total: 8.56 s\n",
      "Wall time: 8.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cpt2ftdist = infer(train_test_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.09892473,  8.97881891,  1.        , ...,  1.        ,\n",
       "        1.        ,  1.        ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpt2ftdist['book']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_prec(cpt2ftdist):\n",
    "    precs_1, precs_5 = [], []\n",
    "    for cpt in cpt2ftdist.iterkeys():\n",
    "        true_fts = cpt2ft[cpt]\n",
    "        pred_fts = map(lambda f_i:features[f_i], np.argsort(cpt2ftdist[cpt])[::-1][:5])\n",
    "        precs_1.append(1 if pred_fts[0] in true_fts else 0)\n",
    "        precs_5.append(sum(1 if pred_ft in true_fts else 0 for pred_ft in pred_fts) / 5)\n",
    "    print \"Average Prec @1: %.6f%%\" % (np.mean(precs_1)*100)\n",
    "    print \"Average Prec @5: %.6f%%\" % (np.mean(precs_5)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Prec @1: 22.222222%\n",
      "Average Prec @5: 14.984127%\n"
     ]
    }
   ],
   "source": [
    "evaluate_prec(cpt2ftdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_map(cpt2ftdist):\n",
    "    aps = []\n",
    "    for cpt in cpt2ftdist.iterkeys():\n",
    "        true_ftbin = np.array([1 if ft in cpt2ft[cpt] else 0 for ft in features])\n",
    "        pred_ftdist = normalize(cpt2ftdist[cpt])\n",
    "        aps.append(average_precision_score(true_ftbin, pred_ftdist))\n",
    "    print \"MAP: %.6f%%\" % (np.mean(aps)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP: 12.465676%\n"
     ]
    }
   ],
   "source": [
    "evaluate_map(cpt2ftdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
