{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "* Multinomial Bimodal Topic Model (Properties $\\sim$ Multinomial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Prepare Input\n",
    "\n",
    "* norms: list of all McRae norms (541)\n",
    "* props: list of all McRae properties (2526)\n",
    "* norm2prop: norm -> prop mapping\n",
    "* corpus triples: (word, dep, head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norms,norm2prop,norm2propprob,props = cPickle.load(open(\"/Users/jacobsw/Desktop/UNIV/FALL_2016/LIN389C_RSCH_COMPLING/BAYESIAN/DATA/MCRAE/mcare_norm_data.p\",'rb'))\n",
    "dep_triples = cPickle.load(open(\"/Users/jacobsw/Desktop/UNIV/FALL_2016/LIN389C_RSCH_COMPLING/BAYESIAN/DATA/BROWN/brown_triples.p\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norms = list(set(norms)) # ignore polysemy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Facilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from itertools import chain\n",
    "from __future__ import division\n",
    "from operator import add\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HELPERs\n",
    "\n",
    "def normalize(arr):\n",
    "    return arr / arr.sum()\n",
    "\n",
    "def partition(l, k):\n",
    "    k = max(1, k)\n",
    "    chunk_size = len(l)//k\n",
    "    return [l[i:i+chunk_size] for i in xrange(0, len(l), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EVALUATORS\n",
    "\n",
    "def topk_evaluate(norm2propdist, k): # test norms.\n",
    "    accuracies = []\n",
    "    for norm in norm2propdist.iterkeys():\n",
    "        true_props = norm2prop[norm]\n",
    "        pred_props = map(lambda idx:props[idx], np.argsort(norm2propdist[norm])[::-1][:k])\n",
    "        accuracy = 0.\n",
    "        for pred_prop in pred_props:\n",
    "            if pred_prop in true_props: \n",
    "                accuracy = 1. # gold-standard prop retrieved, count 'accuracy'.\n",
    "                break\n",
    "        accuracies.append(accuracy)\n",
    "    print \"Percentage Accurate (Test Norms) in Top %d Predictions: %.6f%%\" % (k, np.mean(accuracies)*100)\n",
    "\n",
    "def map_evaluate(norm2propdist):\n",
    "    avg_precs = []\n",
    "    for norm in norm2propdist.iterkeys():\n",
    "        true_props = norm2prop[norm]\n",
    "        pred_props = map(lambda idx:props[idx], np.argsort(norm2propdist[norm])[::-1])\n",
    "        num_correct, precs = 0, []\n",
    "        for i,pred_prop in enumerate(pred_props):\n",
    "            if pred_prop in true_props:\n",
    "                num_correct += 1\n",
    "                precs.append(num_correct/(i+1))\n",
    "        avg_precs.append(np.mean(precs))\n",
    "    print \"MAP: %.6f%%\" % (np.mean(avg_precs)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class WPMBTM:\n",
    "    \n",
    "    def __init__(self, norms, props, norm2prop, norm2propprob, triples):\n",
    "        self.all_norms = norms # train-test split later.\n",
    "        self.props = props\n",
    "        self.n2i = {n:i for i,n in enumerate(self.all_norms)} # indexing pseudo-docs, each corresponds to 1 train norm.\n",
    "        self.f2i = {f:i for i,f in enumerate(self.props)}\n",
    "        self.norm2prop = norm2prop\n",
    "        self.norm2propprob = norm2propprob\n",
    "        self.triples = triples\n",
    "        self.preprocessing()\n",
    " \n",
    "    def preprocessing(self):\n",
    "        # construct norm2vr and norm2f mappings.\n",
    "        all_norms_set = set(self.all_norms) # for fast lookup.\n",
    "        self.norm2vr_f = defaultdict(lambda : defaultdict(list))\n",
    "        for norm,dep,head in self.triples:\n",
    "            if norm in all_norms_set and (dep=='nsubj' or dep=='dobj'):\n",
    "                vr = head+'-'+dep\n",
    "                f = np.random.choice(self.norm2prop[norm],p=self.norm2propprob[norm])\n",
    "                self.norm2vr_f[norm]['vr'].append(vr)\n",
    "                self.norm2vr_f[norm]['f'].append(f)\n",
    "        self.all_norms = self.norm2vr_f.keys() # narrow it down to norms appearing in the current corpus.\n",
    "        \n",
    "    def learn(self, train_norms, topics):\n",
    "        # make pseudo-documents.\n",
    "        vrs = set() \n",
    "        norm2vr_f = deepcopy(self.norm2vr_f) # avoid subtle changes to the original mapping dict.\n",
    "        word_docs, prop_docs = [], [] # word_docs: for v-r pairs.\n",
    "        for norm in train_norms:\n",
    "            word_docs.append(norm2vr_f[norm]['vr'])\n",
    "            prop_docs.append(norm2vr_f[norm]['f'])\n",
    "            vrs = vrs.union(set(norm2vr_f[norm]['vr']))\n",
    "        vrs = list(vrs)\n",
    "        w2i = {vr:i for i,vr in enumerate(vrs)} \n",
    "        for word_doc,prop_doc in zip(word_docs,prop_docs):\n",
    "            for i,(word,prop) in enumerate(zip(word_doc,prop_doc)):\n",
    "                sampled_topic = np.random.choice(topics)\n",
    "                word_doc[i] = (word,sampled_topic)\n",
    "                prop_doc[i] = (prop,sampled_topic)\n",
    "        # topic modeling\n",
    "        print \"... Running Topic Model\"\n",
    "        W, F, D, T = len(vrs), len(self.props), len(word_docs), len(topics)\n",
    "        alpha, beta, gamma = 50/2, .001, .001\n",
    "        alpha_arr = np.array([alpha for _ in range(T)]); Talpha_arr = np.array([alpha*T for _ in range(T)])\n",
    "        beta_arr = np.array([beta for _ in range(T)]); Wbeta_arr = np.array([beta*W for _ in range(T)])\n",
    "        gamma_arr = np.array([gamma for _ in range(T)]); Fgamma_arr = np.array([gamma*F for _ in range(T)])\n",
    "        C_WT, C_FT, C_DT = np.zeros((W,T)), np.zeros((F,T)), np.zeros((D,T))\n",
    "        wt_counts = reduce(add,[Counter(word_doc) for word_doc in word_docs])\n",
    "        ft_counts = reduce(add,[Counter(prop_doc) for prop_doc in prop_docs])\n",
    "        for (w,t_w),wt_count in wt_counts.iteritems(): C_WT[w2i[w]][t_w] = wt_count \n",
    "        for (f,t_f),ft_count in ft_counts.iteritems(): C_FT[self.f2i[f]][t_f] = ft_count\n",
    "        for i,doc in enumerate(word_docs): # word,property pairs have the same topic assignments.\n",
    "            dt_counts = Counter([t for w,t in doc])\n",
    "            for j in range(T):\n",
    "                C_DT[i][j] = dt_counts[j]\n",
    "        if not C_WT.sum()==C_FT.sum()==C_DT.sum():\n",
    "            raise Exception, \"Error in counts in pseudo-document.\"\n",
    "        def sample_topic(w_i,f_i,d): # defined locally to avoid passing around big matrices.\n",
    "            P_num_arr = (C_WT[w_i,:]+beta_arr) * (C_FT[f_i,:]+gamma_arr) * (C_DT[d,:]+alpha_arr)\n",
    "            P_denom_arr = (np.apply_along_axis(sum,0,C_WT)+Wbeta_arr) * \\\n",
    "                          (np.apply_along_axis(sum,0,C_FT)+Fgamma_arr) * \\\n",
    "                          (T * (C_DT[d,:].sum()+T*alpha))\n",
    "            P = normalize(P_num_arr / P_denom_arr)\n",
    "            return np.random.choice(np.array(topics),p=normalize(P)) \n",
    "        def gibbs(n_iters=100, verbose_freq=20): # 30: experimentally where the convergence is achieved.\n",
    "            for e in range(n_iters):\n",
    "                if e!=0 and e%verbose_freq==0: print \"@ %dth iteration\" % e\n",
    "                for d,(word_doc,prop_doc) in enumerate(zip(word_docs,prop_docs)):\n",
    "                    for (w,t),(f,_) in zip(word_doc,prop_doc): # a (w,f) pair have the same topic.\n",
    "                        if C_WT[w2i[w]][t]==0 or C_FT[self.f2i[f]][t]==0 or C_DT[d][t]==0: continue\n",
    "                        C_WT[w2i[w]][t] -= 1\n",
    "                        C_FT[self.f2i[f]][t] -= 1\n",
    "                        C_DT[d][t] -= 1\n",
    "                        new_t = sample_topic(w2i[w],self.f2i[f],d)\n",
    "                        C_WT[w2i[w]][new_t] += 1\n",
    "                        C_FT[self.f2i[f]][new_t] += 1\n",
    "                        C_DT[d][new_t] += 1\n",
    "        gibbs()\n",
    "        return C_WT, C_FT, w2i\n",
    "        \n",
    "    def infer(self, cv=5, topics=range(100)): # cv >= 2\n",
    "        # norm set chunking\n",
    "        random.shuffle(self.all_norms)\n",
    "        norm_chunks = partition(self.all_norms, cv)\n",
    "        self.results = []\n",
    "        for i in range(cv):\n",
    "            print \"... Running CV round %d\" % (i+1)\n",
    "            train_norms = list(chain.from_iterable([norm_chunk for j,norm_chunk in enumerate(norm_chunks)\n",
    "                                                    if j!=i])) # flatten.\n",
    "            test_norms = norm_chunks[i]\n",
    "            C_WT, C_FT, w2i = self.learn(train_norms, topics)\n",
    "            def p_z_given_vr(z, vr):\n",
    "                return C_WT[w2i[vr]][z] / C_WT[w2i[vr],:].sum()\n",
    "            def p_f_given_z(f, z):\n",
    "                return C_FT[self.f2i[f]][z] / C_FT[:,z].sum()\n",
    "            norm2propdist = defaultdict(list)\n",
    "            for norm in test_norms:\n",
    "                vrs = list(set(self.norm2vr_f[norm]['vr']))\n",
    "                p_z_arr = [sum(p_z_given_vr(z,vr) if vr in w2i else 0. for vr in vrs) for z in topics]\n",
    "                p_f_arr = [np.dot([p_f_given_z(f,z) for z in topics],p_z_arr) for f in self.props]\n",
    "                norm2propdist[norm] = p_f_arr\n",
    "            self.results.append(norm2propdist)\n",
    "    \n",
    "    def evaluate(self):\n",
    "        for i,norm2propdist in enumerate(self.results):\n",
    "            print \"CV round %d results:\" % (i+1)\n",
    "            topk_evaluate(norm2propdist, k=1)\n",
    "            topk_evaluate(norm2propdist, k=5)\n",
    "            topk_evaluate(norm2propdist, k=10)\n",
    "            topk_evaluate(norm2propdist, k=20)\n",
    "            map_evaluate(norm2propdist)\n",
    "            print            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CV = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Running CV round 1\n",
      "... Running Topic Model\n",
      "@ 20th iteration\n",
      "@ 40th iteration\n",
      "@ 60th iteration\n",
      "@ 80th iteration\n",
      "... Running CV round 2\n",
      "... Running Topic Model\n",
      "@ 20th iteration\n",
      "@ 40th iteration\n",
      "@ 60th iteration\n",
      "@ 80th iteration\n",
      "... Running CV round 3\n",
      "... Running Topic Model\n",
      "@ 20th iteration\n",
      "@ 40th iteration\n",
      "@ 60th iteration\n",
      "@ 80th iteration\n",
      "... Running CV round 4\n",
      "... Running Topic Model\n",
      "@ 20th iteration\n",
      "@ 40th iteration\n",
      "@ 60th iteration\n",
      "@ 80th iteration\n",
      "... Running CV round 5\n",
      "... Running Topic Model\n",
      "@ 20th iteration\n",
      "@ 40th iteration\n",
      "@ 60th iteration\n",
      "@ 80th iteration\n",
      "CPU times: user 46min 26s, sys: 19 s, total: 46min 45s\n",
      "Wall time: 46min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wpmbtm = WPMBTM(norms, props, norm2prop, norm2propprob, dep_triples)\n",
    "wpmbtm.infer(cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV round 1 results:\n",
      "Percentage Accurate (Test Norms) in Top 1 Predictions: 6.153846%\n",
      "Percentage Accurate (Test Norms) in Top 5 Predictions: 21.538462%\n",
      "Percentage Accurate (Test Norms) in Top 10 Predictions: 35.384615%\n",
      "Percentage Accurate (Test Norms) in Top 20 Predictions: 56.923077%\n",
      "MAP: 3.325931%\n",
      "\n",
      "CV round 2 results:\n",
      "Percentage Accurate (Test Norms) in Top 1 Predictions: 9.230769%\n",
      "Percentage Accurate (Test Norms) in Top 5 Predictions: 33.846154%\n",
      "Percentage Accurate (Test Norms) in Top 10 Predictions: 55.384615%\n",
      "Percentage Accurate (Test Norms) in Top 20 Predictions: 63.076923%\n",
      "MAP: 3.697320%\n",
      "\n",
      "CV round 3 results:\n",
      "Percentage Accurate (Test Norms) in Top 1 Predictions: 9.230769%\n",
      "Percentage Accurate (Test Norms) in Top 5 Predictions: 30.769231%\n",
      "Percentage Accurate (Test Norms) in Top 10 Predictions: 43.076923%\n",
      "Percentage Accurate (Test Norms) in Top 20 Predictions: 58.461538%\n",
      "MAP: 3.653193%\n",
      "\n",
      "CV round 4 results:\n",
      "Percentage Accurate (Test Norms) in Top 1 Predictions: 9.230769%\n",
      "Percentage Accurate (Test Norms) in Top 5 Predictions: 32.307692%\n",
      "Percentage Accurate (Test Norms) in Top 10 Predictions: 52.307692%\n",
      "Percentage Accurate (Test Norms) in Top 20 Predictions: 63.076923%\n",
      "MAP: 4.346863%\n",
      "\n",
      "CV round 5 results:\n",
      "Percentage Accurate (Test Norms) in Top 1 Predictions: 6.153846%\n",
      "Percentage Accurate (Test Norms) in Top 5 Predictions: 30.769231%\n",
      "Percentage Accurate (Test Norms) in Top 10 Predictions: 40.000000%\n",
      "Percentage Accurate (Test Norms) in Top 20 Predictions: 64.615385%\n",
      "MAP: 4.047140%\n",
      "\n",
      "CPU times: user 717 ms, sys: 3.83 ms, total: 721 ms\n",
      "Wall time: 718 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wpmbtm.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### CV = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Running CV round 1\n",
      "... Running Topic Model\n",
      "@ 20th iteration\n",
      "@ 40th iteration\n",
      "@ 60th iteration\n",
      "@ 80th iteration\n",
      "... Running CV round 2\n",
      "... Running Topic Model\n",
      "@ 20th iteration\n",
      "@ 40th iteration\n",
      "@ 60th iteration\n",
      "@ 80th iteration\n",
      "... Running CV round 3\n",
      "... Running Topic Model\n",
      "@ 20th iteration\n",
      "@ 40th iteration\n",
      "@ 60th iteration\n",
      "@ 80th iteration\n",
      "... Running CV round 4\n",
      "... Running Topic Model\n",
      "@ 20th iteration\n",
      "@ 40th iteration\n",
      "@ 60th iteration\n",
      "@ 80th iteration\n",
      "... Running CV round 5\n",
      "... Running Topic Model\n",
      "@ 20th iteration\n",
      "@ 40th iteration\n",
      "@ 60th iteration\n",
      "@ 80th iteration\n",
      "... Running CV round 6\n",
      "... Running Topic Model\n",
      "@ 20th iteration\n",
      "@ 40th iteration\n",
      "@ 60th iteration\n",
      "@ 80th iteration\n",
      "... Running CV round 7\n",
      "... Running Topic Model\n",
      "@ 20th iteration\n",
      "@ 40th iteration\n",
      "@ 60th iteration\n",
      "@ 80th iteration\n",
      "... Running CV round 8\n",
      "... Running Topic Model\n",
      "@ 20th iteration\n",
      "@ 40th iteration\n",
      "@ 60th iteration\n",
      "@ 80th iteration\n",
      "... Running CV round 9\n",
      "... Running Topic Model\n",
      "@ 20th iteration\n",
      "@ 40th iteration\n",
      "@ 60th iteration\n",
      "@ 80th iteration\n",
      "... Running CV round 10\n",
      "... Running Topic Model\n",
      "@ 20th iteration\n",
      "@ 40th iteration\n",
      "@ 60th iteration\n",
      "@ 80th iteration\n",
      "CPU times: user 1h 46min 14s, sys: 56.2 s, total: 1h 47min 10s\n",
      "Wall time: 1h 47min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wpmbtm = WPMBTM(norms, props, norm2prop, norm2propprob, dep_triples)\n",
    "wpmbtm.infer(cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV round 1 results:\n",
      "Percentage Accurate (Test Norms) in Top 1 Predictions: 9.375000%\n",
      "Percentage Accurate (Test Norms) in Top 5 Predictions: 28.125000%\n",
      "Percentage Accurate (Test Norms) in Top 10 Predictions: 37.500000%\n",
      "Percentage Accurate (Test Norms) in Top 20 Predictions: 62.500000%\n",
      "MAP: 3.453744%\n",
      "\n",
      "CV round 2 results:\n",
      "Percentage Accurate (Test Norms) in Top 1 Predictions: 3.125000%\n",
      "Percentage Accurate (Test Norms) in Top 5 Predictions: 31.250000%\n",
      "Percentage Accurate (Test Norms) in Top 10 Predictions: 50.000000%\n",
      "Percentage Accurate (Test Norms) in Top 20 Predictions: 59.375000%\n",
      "MAP: 3.348909%\n",
      "\n",
      "CV round 3 results:\n",
      "Percentage Accurate (Test Norms) in Top 1 Predictions: 9.375000%\n",
      "Percentage Accurate (Test Norms) in Top 5 Predictions: 31.250000%\n",
      "Percentage Accurate (Test Norms) in Top 10 Predictions: 46.875000%\n",
      "Percentage Accurate (Test Norms) in Top 20 Predictions: 53.125000%\n",
      "MAP: 4.508788%\n",
      "\n",
      "CV round 4 results:\n",
      "Percentage Accurate (Test Norms) in Top 1 Predictions: 12.500000%\n",
      "Percentage Accurate (Test Norms) in Top 5 Predictions: 43.750000%\n",
      "Percentage Accurate (Test Norms) in Top 10 Predictions: 53.125000%\n",
      "Percentage Accurate (Test Norms) in Top 20 Predictions: 65.625000%\n",
      "MAP: 4.994976%\n",
      "\n",
      "CV round 5 results:\n",
      "Percentage Accurate (Test Norms) in Top 1 Predictions: 15.625000%\n",
      "Percentage Accurate (Test Norms) in Top 5 Predictions: 34.375000%\n",
      "Percentage Accurate (Test Norms) in Top 10 Predictions: 50.000000%\n",
      "Percentage Accurate (Test Norms) in Top 20 Predictions: 53.125000%\n",
      "MAP: 4.603361%\n",
      "\n",
      "CV round 6 results:\n",
      "Percentage Accurate (Test Norms) in Top 1 Predictions: 6.250000%\n",
      "Percentage Accurate (Test Norms) in Top 5 Predictions: 31.250000%\n",
      "Percentage Accurate (Test Norms) in Top 10 Predictions: 46.875000%\n",
      "Percentage Accurate (Test Norms) in Top 20 Predictions: 56.250000%\n",
      "MAP: 4.245701%\n",
      "\n",
      "CV round 7 results:\n",
      "Percentage Accurate (Test Norms) in Top 1 Predictions: 6.250000%\n",
      "Percentage Accurate (Test Norms) in Top 5 Predictions: 40.625000%\n",
      "Percentage Accurate (Test Norms) in Top 10 Predictions: 53.125000%\n",
      "Percentage Accurate (Test Norms) in Top 20 Predictions: 65.625000%\n",
      "MAP: 4.037005%\n",
      "\n",
      "CV round 8 results:\n",
      "Percentage Accurate (Test Norms) in Top 1 Predictions: 9.375000%\n",
      "Percentage Accurate (Test Norms) in Top 5 Predictions: 43.750000%\n",
      "Percentage Accurate (Test Norms) in Top 10 Predictions: 59.375000%\n",
      "Percentage Accurate (Test Norms) in Top 20 Predictions: 84.375000%\n",
      "MAP: 4.616027%\n",
      "\n",
      "CV round 9 results:\n",
      "Percentage Accurate (Test Norms) in Top 1 Predictions: 18.750000%\n",
      "Percentage Accurate (Test Norms) in Top 5 Predictions: 37.500000%\n",
      "Percentage Accurate (Test Norms) in Top 10 Predictions: 43.750000%\n",
      "Percentage Accurate (Test Norms) in Top 20 Predictions: 56.250000%\n",
      "MAP: 4.771279%\n",
      "\n",
      "CV round 10 results:\n",
      "Percentage Accurate (Test Norms) in Top 1 Predictions: 6.250000%\n",
      "Percentage Accurate (Test Norms) in Top 5 Predictions: 34.375000%\n",
      "Percentage Accurate (Test Norms) in Top 10 Predictions: 34.375000%\n",
      "Percentage Accurate (Test Norms) in Top 20 Predictions: 53.125000%\n",
      "MAP: 4.292655%\n",
      "\n",
      "CPU times: user 929 ms, sys: 10.1 ms, total: 939 ms\n",
      "Wall time: 940 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wpmbtm.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
