{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent Beta Count (Syntax-Context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0a. Coding Info\n",
    "\n",
    "* **Corpus**\n",
    "    * Brown\n",
    "    * McRae Norms\n",
    "* **Parser**\n",
    "    * SpaCy\n",
    "* **Algorithm**\n",
    "    * Count-based Indep. Beta Bayesian Updating (Erk 2016)\n",
    "* **Evaluation**\n",
    "    * Precision @1/@5\n",
    "    * Mean Average Precision (MAP) https://www.youtube.com/watch?v=pM6DJ0ZZee0&index=12&list=PLBv09BD7ez_6nqE9YU9bQXpjJ5jJ1Kgr9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0b. Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "from __future__ import division\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0c. Input Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "concepts,cpt2ft,cpt2ftprob,features = cPickle.load(open(\"/Users/jacobsw/Desktop/UNIV/FALL_2016/LIN389C_RSCH_COMPLING/BAYESIAN/DATA/MCRAE/mcare_norm_data_bern.p\",'rb'))\n",
    "triples = cPickle.load(open(\"/Users/jacobsw/Desktop/UNIV/FALL_2016/LIN389C_RSCH_COMPLING/BAYESIAN/DATA/BROWN/brown_triples.p\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "concepts = list(set(concepts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f2i = {f:i for i,f in enumerate(features)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cpt2ftprob_dic = defaultdict(dict)\n",
    "for cpt in concepts:\n",
    "    cpt2ftprob_dic[cpt] = {ft:ftprob for ft,ftprob in zip(cpt2ft[cpt],cpt2ftprob[cpt])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'found_in_libraries': 0.36666666666666664,\n",
       " 'found_in_schools': 0.23333333333333334,\n",
       " 'found_on_shelves': 0.16666666666666666,\n",
       " 'has_a_hard_cover': 0.33333333333333331,\n",
       " 'has_a_soft_cover': 0.20000000000000001,\n",
       " 'has_authors': 0.43333333333333335,\n",
       " 'has_information': 0.16666666666666666,\n",
       " 'has_page_numbers': 0.16666666666666666,\n",
       " 'has_pages': 0.76666666666666672,\n",
       " 'has_pictures': 0.16666666666666666,\n",
       " 'has_words_in_it': 0.46666666666666667,\n",
       " 'inbeh_-_tells_stories': 0.26666666666666666,\n",
       " 'made_of_paper': 0.29999999999999999,\n",
       " 'used_by_reading': 0.56666666666666665,\n",
       " 'used_for_acquiring/storing_knowledge': 0.16666666666666666,\n",
       " 'used_for_learning': 0.16666666666666666}"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpt2ftprob_dic['book']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0f. General Purpose Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize(arr):\n",
    "    return arr / arr.sum()\n",
    "\n",
    "def partition(l, k):\n",
    "    \"\"\" args: list, # partitions.\n",
    "        returns: a list of k partitions.\n",
    "    \"\"\"\n",
    "    k = max(1, k)\n",
    "    chunk_size = len(l)//k\n",
    "    if len(l)%k==0:\n",
    "        return [l[i:i+chunk_size] for i in xrange(0, len(l), chunk_size)]\n",
    "    return [l[i:i+chunk_size] if idx<k-1 else l[i:] for idx,i in enumerate(xrange(0, len(l), chunk_size))][:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Mappings\n",
    "\n",
    "* Verb-Role $\\mapsto$ Concept List/Set Mapping\n",
    "* Concept $\\mapsto$ Verb-Role List/Set Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preproc_mapping(concepts, triples):\n",
    "    \"\"\" args: concepts, features, concept->feature mapping, concept->P(feature) mapping, dependency triples.\n",
    "        returns: verb-role->concept-set mapping.\n",
    "    \"\"\"\n",
    "    concepts_set = set(concepts) # for fast lookup.\n",
    "    vr2c = defaultdict(set)\n",
    "    c2vr = defaultdict(list)\n",
    "    for word,dep,head in triples:\n",
    "        if word in concepts_set and (dep=='nsubj' or dep=='dobj'):\n",
    "            vr2c[head+'-'+dep].add(word)\n",
    "            c2vr[word].append(head+'-'+dep)\n",
    "    return vr2c, c2vr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vr2c, c2vr = preproc_mapping(concepts, triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "975\n"
     ]
    }
   ],
   "source": [
    "vrs = vr2c.keys() # global verb-role pair indexing.\n",
    "vr2i = {vr:i for i,vr in enumerate(vrs)}\n",
    "print len(vrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327\n"
     ]
    }
   ],
   "source": [
    "concepts_subset = c2vr.keys() # ORDER DOESN'T MATTER\n",
    "print len(concepts_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preproc_split(concepts, k=5):\n",
    "    \"\"\" arg: concepts, cv numbers.\n",
    "        returns: k .8/.2 train-test splits.\n",
    "    \"\"\"\n",
    "    # random.shuffle(concepts) # TODO: messing up with the indexing. will fix later.\n",
    "    concept_chunks = partition(concepts, k)\n",
    "    train_test_splits = []\n",
    "    for i in range(k):\n",
    "        train_cpts = list(chain.from_iterable([concept_chunk for j,concept_chunk in enumerate(concept_chunks)\n",
    "                                               if j!=i]))\n",
    "        test_cpts = concept_chunks[i]\n",
    "        train_test_splits.append((train_cpts,test_cpts))\n",
    "    return train_test_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test_splits = preproc_split(concepts_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'wagon', u'bat', u'clam', u'peacock', u'onion', u'level', u'cigar', u'bracelet', u'gun', u'glove', u'swimsuit', u'axe', u'bag', u'microscope', u'hatchet', u'subway', u'rock', u'nightingale', u'mouse', u'garage', u'tray', u'trailer', u'lantern', u'turtle', u'butterfly', u'fence', u'cage', u'house', u'duck', u'crown', u'tongs', u'worm', u'lion', u'trouser', u'cellar', u'mirror', u'candle', u'marble', u'hammer', u'chicken', u'whip', u'barn', u'moth', u'peg', u'closet', u'clock', u'kettle', u'fox', u'revolver', u'vest', u'mug', u'stool', u'bureau', u'pistol', u'pea', u'knife', u'tent', u'cockroach', u'sheep', u'pepper', u'hose', u'umbrella', u'pencil', u'camel', u'pear', u'corn', u'slingshot', u'cart', u'cork', u'guitar', u'sandal', u'spoon', u'grasshopper', u'bolt', u'key', u'goldfish', u'comb', u'grenade', u'card', u'box', u'stone', u'drum', u'jet', u'cow', u'saddle', u'jeep', u'walrus', u'biscuit', u'anchor', u'magazine', u'cod', u'falcon', u'ox', u'coin', u'buckle', u'scarf', u'sink', u'clamp', u'shovel', u'apple', u'spear', u'toilet', u'wall', u'jar', u'pot', u'bayonet', u'coconut', u'sack', u'cloak', u'slipper', u'vine', u'brush', u'church', u'table', u'ring', u'boat', u'belt', u'turkey', u'razor', u'horse', u'toy', u'van', u'certificate', u'pearl', u'dove', u'sweater', u'doll', u'sock', u'frog', u'submarine', u'crow', u'curtain', u'brick', u'limousine', u'banana', u'calf', u'fork', u'tomato', u'shell', u'door', u'squirrel', u'shelf', u'shawl', u'buggy', u'envelope', u'faucet', u'tripod', u'cabinet', u'hut', u'orange', u'train', u'stick', u'rabbit', u'rifle', u'cottage', u'coyote', u'crane', u'rooster', u'radish', u'robe', u'tortoise', u'tank', u'pant', u'car', u'ruler', u'cap', u'chisel', u'rat', u'bed', u'cat', u'rope', u'cabin', u'drill', u'coat', u'cake', u'bathtub', u'plug', u'tractor', u'shrimp', u'bridge', u'donkey', u'mink', u'pony', u'pin', u'canoe', u'pie', u'seal', u'deer', u'telephone', u'bus', u'pig', u'ant', u'inn', u'carrot', u'cathedral', u'violin', u'ship', u'bow', u'skirt', u'airplane', u'spade', u'lamb', u'jacket', u'lemon', u'shirt', u'peach', u'mole', u'dress', u'balloon', u'bowl', u'couch', u'lamp', u'book', u'caterpillar', u'parakeet', u'sword', u'elephant', u'tie', u'saucer', u'mitten', u'goat', u'pan', u'typewriter', u'wheel', u'ball', u'cupboard', u'mat', u'snail', u'screw', u'toad', u'football', u'pineapple', u'bucket', u'cigarette', u'tack', u'garlic', u'salamander', u'apron', u'cabbage', u'missile', u'medal', u'jean', u'building', u'stereo', u'sardine', u'bullet', u'potato', u'shotgun', u'urn', u'napkin', u'hawk', u'dog', u'pillow', u'pipe', u'truck', u'bottle', u'ambulance', u'basket', u'barrel', u'buffalo', u'eagle', u'drain']\n",
      "\n",
      "[u'taxi', u'gown', u'bull', u'bomb', u'kite', u'rocket', u'owl', u'boot', u'stove', u'desk', u'doorknob', u'rattlesnake', u'crocodile', u'rattle', u'radio', u'shoe', u'oven', u'bedroom', u'dish', u'pheasant', u'chair', u'helicopter', u'carpet', u'muzzle', u'pigeon', u'apartment', u'grape', u'chain', u'cup', u'basement', u'menu', u'broom', u'swan', u'avocado', u'pen', u'bench', u'celery', u'bouquet', u'thimble', u'tape', u'board', u'wasp', u'bin', u'piano', u'rice', u'pumpkin', u'sailboat', u'plate', u'tap', u'lime', u'python', u'cannon', u'thermometer', u'vulture', u'bear', u'baton', u'whistle', u'bean', u'fan', u'cushion', u'gate', u'flea', u'cape', u'banner', u'bread']\n",
      "\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print train_test_splits[0][0]; print\n",
    "print train_test_splits[0][1]; print\n",
    "print len(train_test_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** ================== NB: The objects above this line are globally referenced!! ==================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_ft_vec(cpt):\n",
    "    \"\"\" args: a concept\n",
    "        returns: a len(features) array of feature probs in numpy array.\n",
    "    \"\"\"\n",
    "    return np.array([cpt2ftprob_dic[cpt][ft] if ft in cpt2ftprob_dic[cpt] else 0 for ft in features],dtype=np.float32)\n",
    "\n",
    "def learn_stc(train_cpts):\n",
    "    \n",
    "    vr2ftabs = defaultdict(lambda : np.array([[1,1] for _ in range(len(features))],dtype=np.float32))\n",
    "    for vr in vrs:\n",
    "        cpts = list(vr2c[vr])\n",
    "        for cpt in filter(lambda cpt:cpt in train_cpts, cpts):\n",
    "            cpt_ftprobs = to_ft_vec(cpt)\n",
    "            for f_i in range(len(features)):\n",
    "                if cpt_ftprobs[f_i]!=0:\n",
    "                    vr2ftabs[vr][f_i] = [vr2ftabs[vr][f_i][0]+cpt_ftprobs[f_i], \n",
    "                                         vr2ftabs[vr][f_i][1]+(1-cpt_ftprobs[f_i])]\n",
    "                else:\n",
    "                    vr2ftabs[vr][f_i] = [vr2ftabs[vr][f_i][0], \n",
    "                                         vr2ftabs[vr][f_i][1]+1]                    \n",
    "    return vr2ftabs\n",
    "\n",
    "# def learn_bin(train_cpts):\n",
    "\n",
    "#     vr2ftabs = defaultdict(lambda : np.array([[1,1] for _ in range(len(features))],dtype=np.float32))\n",
    "#     for vr in vrs:\n",
    "#         cpts = list(vr2c[vr])\n",
    "#         for cpt in filter(lambda cpt:cpt in train_cpts, cpts):\n",
    "#             cpt_ftprobs = to_ft_vec(cpt)\n",
    "#             for f_i in range(len(features)):\n",
    "#                 if cpt_ftprobs[f_i]!=0:\n",
    "#                     vr2ftabs[vr][f_i] = [vr2ftabs[vr][f_i][0]+1, vr2ftabs[vr][f_i][0]] # if pos, alpha+1\n",
    "#                 else:\n",
    "#                     vr2ftabs[vr][f_i] = [vr2ftabs[vr][f_i][0], vr2ftabs[vr][f_i][1]+1] # if neg, beta+1\n",
    "#     return vr2ftabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# t = learn_bin(train_test_splits[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i,(a,b) in enumerate(t['feed-dobj']):\n",
    "#     if a!=1:\n",
    "#         print a,b,features[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = learn_stc(train_test_splits[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for i,(a,b) in enumerate(t['feed-dobj']):\n",
    "#     print a,b,features[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def infer_stc(train_test_splits):\n",
    "    \n",
    "    cpt2ftabs = defaultdict(lambda : np.array([[1,1] for _ in range(len(features))],dtype=np.float32))\n",
    "    for train_cpts,test_cpts in train_test_splits:\n",
    "        vr2ftabs = learn_stc(train_cpts)\n",
    "        for cpt in test_cpts:\n",
    "            ftabs = np.array([[1,1] for _ in range(len(features))],dtype=np.float32)\n",
    "            for vr in c2vr[cpt]:\n",
    "                vr_ftabs = vr2ftabs[vr]\n",
    "                for f_i in range(len(features)):\n",
    "                    ftabs[f_i] = [ftabs[f_i][0]+vr_ftabs[f_i][0],\n",
    "                                  ftabs[f_i][1]+vr_ftabs[f_i][1]]\n",
    "            cpt2ftabs[cpt] = ftabs\n",
    "    return cpt2ftabs\n",
    "\n",
    "# def infer_bin(train_test_splits):\n",
    "    \n",
    "#     cpt2ftabs = defaultdict(lambda : np.array([[1,1] for _ in range(len(features))],dtype=np.float32))\n",
    "#     for train_cpts,test_cpts in train_test_splits:\n",
    "#         vr2ftabs = learn_bin(train_cpts)\n",
    "#         for cpt in test_cpts:\n",
    "#             ftabs = np.array([[1,1] for _ in range(len(features))],dtype=np.float32)\n",
    "#             for vr in c2vr[cpt]:\n",
    "#                 vr_ftabs = vr2ftabs[vr]\n",
    "#                 for f_i in range(len(features)):\n",
    "#                     if not (vr_ftabs[f_i][0]==1 and vr_ftabs[f_i][1]==1): # means there's evidence at this feature.\n",
    "#                         ftabs[f_i] = [ftabs[f_i][0]+vr_ftabs[f_i][0],\n",
    "#                                       ftabs[f_i][1]+vr_ftabs[f_i][1]]                       \n",
    "#             cpt2ftabs[cpt] = ftabs\n",
    "#     return cpt2ftabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 20s, sys: 750 ms, total: 2min 20s\n",
      "Wall time: 2min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cpt2ftabs_stc = infer_stc(train_test_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 59s, sys: 1.41 s, total: 3min\n",
      "Wall time: 3min 1s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# cpt2ftabs_bin = infer_bin(train_test_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i,(a,b) in enumerate(cpt2ftabs_bin['cat']):\n",
    "#     if a>b:\n",
    "#         print a,b, a/b, features[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for i,(a,b) in enumerate(cpt2ftabs_bin['cat']):\n",
    "#     if a<b:\n",
    "#         print a,b, a/b, features[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for i,(a,b) in enumerate(cpt2ftabs_stc['cat']):\n",
    "#     if a>b:\n",
    "#         print a,b, a/b, features[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for i,(a,b) in enumerate(cpt2ftabs_stc['cat']):\n",
    "#     if a<b:\n",
    "#         print a,b, a/b, features[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_probdist(cpt2ftabs):\n",
    "    cpt2ftdist = defaultdict(lambda : np.zeros(len(features)))\n",
    "    for cpt in cpt2ftabs.iterkeys():\n",
    "        cpt2ftdist[cpt] = np.array([cpt2ftabs[cpt][f_i][0]/(cpt2ftabs[cpt][f_i][0]+cpt2ftabs[cpt][f_i][1])\n",
    "                                    for f_i in range(len(features))])\n",
    "    return cpt2ftdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cpt2ftdist_stc = to_probdist(cpt2ftabs_stc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cpt2ftdist_bin = to_probdist(cpt2ftabs_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_prec(cpt2ftdist):\n",
    "    precs_1, precs_5 = [], []\n",
    "    for cpt in cpt2ftdist.iterkeys():\n",
    "        true_fts = cpt2ft[cpt]\n",
    "        pred_fts = map(lambda f_i:features[f_i], np.argsort(cpt2ftdist[cpt])[::-1][:5])\n",
    "        precs_1.append(1 if pred_fts[0] in true_fts else 0)\n",
    "        precs_5.append(sum(1 if pred_ft in true_fts else 0 for pred_ft in pred_fts) / 5)\n",
    "    print \"Average Prec @1: %.6f%%\" % (np.mean(precs_1)*100)\n",
    "    print \"Average Prec @5: %.6f%%\" % (np.mean(precs_5)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_map(cpt2ftdist):\n",
    "    aps = []\n",
    "    for cpt in cpt2ftdist.iterkeys():\n",
    "        true_ftbin = np.array([1 if ft in cpt2ft[cpt] else 0 for ft in features])\n",
    "        pred_ftdist = normalize(cpt2ftdist[cpt])\n",
    "        aps.append(average_precision_score(true_ftbin, pred_ftdist))\n",
    "    print \"MAP: %.6f%%\" % (np.mean(aps)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Prec @1: 19.571865%\n",
      "Average Prec @5: 15.963303%\n",
      "MAP: 12.274833%\n"
     ]
    }
   ],
   "source": [
    "evaluate_prec(cpt2ftdist_stc)\n",
    "evaluate_map(cpt2ftdist_stc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# evaluate_prec(cpt2ftdist_bin)\n",
    "# evaluate_map(cpt2ftdist_bin)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
