{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "* I. Single Sentence Learning ($\\alpha=1$)\n",
    "* II. Multiple Sentence Learning ($\\alpha=1$)\n",
    "* III. $\\alpha=0.1$ Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Single Sentence Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Evaluation Scheme 1\n",
    "\n",
    "* Return top 5 most likely property, each accurate prediction accounts for 20% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from nltk.corpus import brown\n",
    "from spacy.en import English\n",
    "from collections import defaultdict\n",
    "from scipy.stats import entropy\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"/Users/jacobsw/Desktop/CODER/IMPLEMENTATION_CAMP/BASIC_TOPICS/DISTRIBUTIONAL_SEMANTICS/DATA/McRae-BRM-InPress/\"\n",
    "filename = \"CONCS_FEATS_concstats_brm.xls\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path+filename, delimiter='\\t')\n",
    "parser = English()\n",
    "brown_sents = [unicode(' '.join(sent)) for sent in brown.sents()]\n",
    "parsed_sents = [parser(sent) for sent in brown_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Evaluator_Single_Learning:\n",
    "    \n",
    "    def __init__(self, parsed_sents, ev_norms, v_argtype_pairs, scheme=1):\n",
    "        \n",
    "        self.brown_lemmas, self.brown_t2l = self.make_token2lemma_dict(parsed_sents)\n",
    "        self.dep_triples = self.extract_dep_triples(parsed_sents)\n",
    "        \n",
    "        self.norms = {df.ix[i]['Concept'] for i in range(df.shape[0])}\n",
    "        self.features = {df.ix[i]['Feature'] for i in range(df.shape[0])}\n",
    "        self.f2i = {f:i for i,f in enumerate(self.features)}\n",
    "        self.feature_list = list(self.features) \n",
    "        \n",
    "        self.ev_norms = ev_norms\n",
    "        self.v_argtype_pairs = v_argtype_pairs\n",
    "        \n",
    "        self.scheme = scheme\n",
    "\n",
    "    ####################\n",
    "    #### EVALUATION ####\n",
    "    ####################\n",
    "    \n",
    "    def evaluate(self):\n",
    "        brown_t2l=self.brown_t2l; dep_triples=self.dep_triples\n",
    "        f2i=self.f2i; feature_list=self.feature_list; norms=self.norms\n",
    "        ev_norms=self.ev_norms; v_argtype_pairs=self.v_argtype_pairs\n",
    "        assert len(ev_norms)==len(v_argtype_pairs)\n",
    "        accuracies = []\n",
    "        for norm,(v,argtype) in zip(ev_norms,v_argtype_pairs):\n",
    "            accuracy = self.single_evaluate(brown_t2l, dep_triples, f2i, feature_list, norms,\n",
    "                                            norm, v, argtype)\n",
    "            accuracies.append(accuracy)\n",
    "        \n",
    "        return accuracies\n",
    "        \n",
    "    def single_evaluate(self, brown_t2l, dep_triples, f2i, feature_list, norms, # database\n",
    "                        out_word, v, argtype): # new input\n",
    "    \n",
    "        norms = filter(lambda norm: norm!=out_word, map(lambda norm:self.norm_normalize(norm,brown_t2l),norms))\n",
    "        norms = map(lambda norm:self.norm_normalize(norm, brown_t2l), norms)\n",
    "        norms_set = set(norms) \n",
    "        norm2prop = self.make_norm2prop(df,brown_t2l)  \n",
    "        \n",
    "        P = len(feature_list)\n",
    "        V = self.training(dep_triples, norm2prop, norms_set, f2i, P) \n",
    "        \n",
    "        w_beta = self.single_update(P, V, v, argtype)\n",
    "        predicted_props = self.top5_props(v, w_beta, feature_list)\n",
    "        \n",
    "        print out_word, predicted_props\n",
    "        \n",
    "        if self.scheme==1:\n",
    "            accuracy = 0\n",
    "            for prop in predicted_props:\n",
    "                if prop in norm2prop[out_word]: accuracy += .2\n",
    "            print \"Word: %s | Accuracy: %.6f%%\" % (out_word, accuracy*100)\n",
    "            return accuracy\n",
    "        else:\n",
    "            accuracy = 0\n",
    "            for prop in predicted_props:\n",
    "                if prop in norm2prop[out_word]: \n",
    "                    accuracy += 1\n",
    "                    print \"Word: %s | Accuracy: %.6f%%\" % (out_word, accuracy*100)\n",
    "                    return accuracy\n",
    "                else: continue\n",
    "            print \"Word: %s | Accuracy: %.6f%%\" % (out_word, accuracy*100)\n",
    "            return accuracy\n",
    "    \n",
    "    def single_update(self, P, V, v, argtype): # argtype = {subj, obj}\n",
    "        w_beta = self.initialize_propdist(P)\n",
    "        v_alphas = V[v][argtype] # alpha as a list\n",
    "        for i,alpha_i in enumerate(v_alphas):\n",
    "            if alpha_i!=1: # i.e. the prop at the position has been seen\n",
    "                w_beta[i] += alpha_i\n",
    "        return w_beta \n",
    "\n",
    "    def top5_props(self, v, w_beta, feature_list):\n",
    "        w_beta = w_beta / w_beta.sum()\n",
    "        avg_ent = entropy(w_beta)\n",
    "        top5_props_idx = np.argsort(w_beta)[::-1][:5]\n",
    "        top5_props_names = map(lambda idx:feature_list[idx], top5_props_idx)\n",
    "        return top5_props_names\n",
    "\n",
    "    ###################\n",
    "    #### FUNCTIONS ####\n",
    "    ###################\n",
    "    \n",
    "    def training(self, triples, norm2prop, norms_set, f2i, P):\n",
    "        V = defaultdict(lambda : defaultdict(lambda : self.initialize_propdist(P)))\n",
    "        for triple in triples:\n",
    "            if triple[1].endswith('subj') and triple[0] in norms_set:\n",
    "                props = self.get_props(norm2prop, triple[0])\n",
    "                for prop in props:\n",
    "                    V[triple[2]]['subj'][f2i[prop]] += 1\n",
    "            elif triple[1].endswith('obj') and triple[0] in norms_set:\n",
    "                props = self.get_props(norm2prop, triple[0])\n",
    "                for prop in props:\n",
    "                    V[triple[2]]['obj'][f2i[prop]] += 1\n",
    "            else: pass\n",
    "        return V    \n",
    "    \n",
    "    def make_token2lemma_dict(self, parsed_sents):\n",
    "        lemmas = set()\n",
    "        token2lemma = {}\n",
    "        for parsed_sent in parsed_sents:\n",
    "            for token in parsed_sent:\n",
    "                token2lemma[token.orth_] = token.lemma_\n",
    "                lemmas.add(token.lemma_)\n",
    "        return lemmas, token2lemma\n",
    "    \n",
    "    def norm_normalize(self, norm, t2l):\n",
    "        norm = norm.split('_')[0] if '_' in norm else norm\n",
    "        if norm in t2l: return t2l[norm]\n",
    "        return norm    \n",
    "    \n",
    "    def make_norm2prop(self, df, t2l):\n",
    "        norm2prop = defaultdict(list)\n",
    "        for i in xrange(df.shape[0]):\n",
    "            norm2prop[self.norm_normalize(df['Concept'][i],t2l)].append(df['Feature'][i]) \n",
    "        return norm2prop\n",
    "\n",
    "    def extract_dep_triples(self, parsed_sents):\n",
    "        triples = []\n",
    "        for parsed_sent in parsed_sents:\n",
    "            for token in parsed_sent:\n",
    "                lemma_triple = (token.lemma_, token.dep_, token.head.lemma_)\n",
    "                triples.append(lemma_triple)\n",
    "        return triples\n",
    "    \n",
    "    def initialize_propdist(self, P): \n",
    "        return np.zeros(P) + .1\n",
    "    \n",
    "    def get_props(self, norm2prop, w):\n",
    "        return norm2prop[w]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"/Users/jacobsw/Desktop/UNIV/FALL_2016/LIN389C_RSCH_COMPLING/BAYESIAN/CODE_DRAFTS/DATA/\"\n",
    "norm2mostcommonpair = cPickle.load(open(path+\"norm2mostcommonpair.p\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'taxi', (u'enter', 'obj')),\n",
       " (u'gown', (u'wear', 'obj')),\n",
       " (u'bomb', (u'drop', 'obj')),\n",
       " (u'kite', (u'fly', 'obj')),\n",
       " (u'rocket', (u'build', 'obj'))]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm2mostcommonpair.items()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ev_norms = []\n",
    "v_argtype_pairs = []\n",
    "for norm,(v,argtype) in norm2mostcommonpair.iteritems():\n",
    "    ev_norms.append(norm)\n",
    "    v_argtype_pairs.append((v,argtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317 317\n"
     ]
    }
   ],
   "source": [
    "print len(ev_norms), len(v_argtype_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.74 s, sys: 217 ms, total: 4.96 s\n",
      "Wall time: 4.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ev_sg = Evaluator_Single_Learning(parsed_sents, ev_norms, v_argtype_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "single_accuracies = ev_sg.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Sentence Learning\n",
      "Average Accuracy: 10.914826%\n",
      "STD: 0.196688\n"
     ]
    }
   ],
   "source": [
    "print \"Single Sentence Learning\"\n",
    "print \"Average Accuracy: %.6f%%\" % (np.mean(single_accuracies)*100)\n",
    "print \"STD: %.6f\" % np.std(single_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Evaluation Scheme 2\n",
    "\n",
    "* Return top 5 most likely property, record 100% accuracy if either of the 5 is accurate, 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.77 s, sys: 206 ms, total: 4.98 s\n",
      "Wall time: 4.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ev_sg2 = Evaluator_Single_Learning(parsed_sents, ev_norms, v_argtype_pairs, scheme=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "single_accuracies2 = ev_sg2.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Sentence Learning\n",
      "Average Accuracy: 31.230284%\n",
      "STD: 0.463433\n"
     ]
    }
   ],
   "source": [
    "print \"Single Sentence Learning\"\n",
    "print \"Average Accuracy: %.6f%%\" % (np.mean(single_accuracies2)*100)\n",
    "print \"STD: %.6f\" % np.std(single_accuracies2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Multiple Sentence Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Evaluation Scheme 1\n",
    "\n",
    "* See above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm2verbdep = cPickle.load(open(path+\"norm2verbdep.p\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Evaluator_Multiple_Learning:\n",
    "    \n",
    "    def __init__(self, parsed_sents, norm2verbdep, scheme=1):\n",
    "        \n",
    "        self.brown_lemmas, self.brown_t2l = self.make_token2lemma_dict(parsed_sents)\n",
    "        self.dep_triples = self.extract_dep_triples(parsed_sents)\n",
    "        \n",
    "        self.norms = {df.ix[i]['Concept'] for i in range(df.shape[0])}\n",
    "        self.features = {df.ix[i]['Feature'] for i in range(df.shape[0])}\n",
    "        self.f2i = {f:i for i,f in enumerate(self.features)}\n",
    "        self.feature_list = list(self.features) \n",
    "        \n",
    "        self.norm2verbdep = norm2verbdep\n",
    "        \n",
    "        self.scheme = scheme\n",
    "\n",
    "    ####################\n",
    "    #### EVALUATION ####\n",
    "    ####################\n",
    "    \n",
    "    def evaluate(self):\n",
    "        brown_t2l=self.brown_t2l; dep_triples=self.dep_triples\n",
    "        f2i=self.f2i; feature_list=self.feature_list; norms=self.norms\n",
    "        norm2verbdep = self.norm2verbdep\n",
    "        accuracies = []\n",
    "        for norm in norm2verbdep.iterkeys():\n",
    "            \n",
    "            accuracy = self.multiple_evaluate(brown_t2l, dep_triples, f2i, feature_list, norms,\n",
    "                                            norm, norm2verbdep[norm])\n",
    "            accuracies.append(accuracy)\n",
    "        \n",
    "        return accuracies\n",
    "\n",
    "    \n",
    "    def multiple_evaluate(self, brown_t2l, dep_triples, f2i, feature_list, norms, # database\n",
    "                        out_word, v_argtype_pairs): # new input\n",
    "    \n",
    "        norms = filter(lambda norm: norm!=out_word, map(lambda norm:self.norm_normalize(norm,brown_t2l),norms))\n",
    "        norms = map(lambda norm:self.norm_normalize(norm, brown_t2l), norms)\n",
    "        norms_set = set(norms) \n",
    "        norm2prop = self.make_norm2prop(df,brown_t2l)  \n",
    "        \n",
    "        P = len(feature_list)\n",
    "        V = self.training(dep_triples, norm2prop, norms_set, f2i, P) \n",
    "        w_beta = self.initialize_propdist(P)\n",
    "        for v,argtype in v_argtype_pairs:\n",
    "            w_beta = self.single_update(P, V, w_beta, v, argtype)\n",
    "        \n",
    "        predicted_props = self.top5_props(v, w_beta, feature_list)\n",
    "        \n",
    "        print out_word, predicted_props\n",
    "        \n",
    "        if self.scheme==1:\n",
    "            accuracy = 0\n",
    "            for prop in predicted_props:\n",
    "                if prop in norm2prop[out_word]: accuracy += .2\n",
    "            print \"Word: %s | Accuracy: %.6f%%\" % (out_word, accuracy*100)\n",
    "            return accuracy\n",
    "        else:\n",
    "            accuracy = 0\n",
    "            for prop in predicted_props:\n",
    "                if prop in norm2prop[out_word]: \n",
    "                    accuracy += 1\n",
    "                    print \"Word: %s | Accuracy: %.6f%%\" % (out_word, accuracy*100)\n",
    "                    return accuracy\n",
    "                else: continue\n",
    "            print \"Word: %s | Accuracy: %.6f%%\" % (out_word, accuracy*100)\n",
    "            return accuracy        \n",
    "\n",
    "    def single_update(self, P, V, w_beta, v, argtype): # argtype = {subj, obj}\n",
    "        v_alphas = V[v][argtype] # alpha as a list\n",
    "        for i,alpha_i in enumerate(v_alphas):\n",
    "            if alpha_i!=1: # i.e. the prop at the position has been seen\n",
    "                w_beta[i] += alpha_i\n",
    "        return w_beta  \n",
    "    \n",
    "    def top5_props(self, v, w_beta, feature_list):\n",
    "        w_beta = w_beta / w_beta.sum()\n",
    "        avg_ent = entropy(w_beta)\n",
    "        top5_props_idx = np.argsort(w_beta)[::-1][:5]\n",
    "        top5_props_names = map(lambda idx:feature_list[idx], top5_props_idx)\n",
    "        return top5_props_names\n",
    "\n",
    "    ###################\n",
    "    #### FUNCTIONS ####\n",
    "    ###################\n",
    "    \n",
    "    def training(self, triples, norm2prop, norms_set, f2i, P):\n",
    "        V = defaultdict(lambda : defaultdict(lambda : self.initialize_propdist(P)))\n",
    "        for triple in triples:\n",
    "            if triple[1].endswith('subj') and triple[0] in norms_set:\n",
    "                props = self.get_props(norm2prop, triple[0])\n",
    "                for prop in props:\n",
    "                    V[triple[2]]['subj'][f2i[prop]] += 1\n",
    "            elif triple[1].endswith('obj') and triple[0] in norms_set:\n",
    "                props = self.get_props(norm2prop, triple[0])\n",
    "                for prop in props:\n",
    "                    V[triple[2]]['obj'][f2i[prop]] += 1\n",
    "            else: pass\n",
    "        return V    \n",
    "    \n",
    "    def make_token2lemma_dict(self, parsed_sents):\n",
    "        lemmas = set()\n",
    "        token2lemma = {}\n",
    "        for parsed_sent in parsed_sents:\n",
    "            for token in parsed_sent:\n",
    "                token2lemma[token.orth_] = token.lemma_\n",
    "                lemmas.add(token.lemma_)\n",
    "        return lemmas, token2lemma\n",
    "    \n",
    "    def norm_normalize(self, norm, t2l):\n",
    "        norm = norm.split('_')[0] if '_' in norm else norm\n",
    "        if norm in t2l: return t2l[norm]\n",
    "        return norm    \n",
    "    \n",
    "    def make_norm2prop(self, df, t2l):\n",
    "        norm2prop = defaultdict(list)\n",
    "        for i in xrange(df.shape[0]):\n",
    "            norm2prop[self.norm_normalize(df['Concept'][i],t2l)].append(df['Feature'][i]) \n",
    "        return norm2prop\n",
    "\n",
    "    def extract_dep_triples(self, parsed_sents):\n",
    "        triples = []\n",
    "        for parsed_sent in parsed_sents:\n",
    "            for token in parsed_sent:\n",
    "                lemma_triple = (token.lemma_, token.dep_, token.head.lemma_)\n",
    "                triples.append(lemma_triple)\n",
    "        return triples\n",
    "    \n",
    "    def initialize_propdist(self, P): \n",
    "        return np.zeros(P) + .1\n",
    "    \n",
    "    def get_props(self, norm2prop, w):\n",
    "        return norm2prop[w]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.9 s, sys: 175 ms, total: 5.07 s\n",
      "Wall time: 5.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ev_ml = Evaluator_Multiple_Learning(parsed_sents, norm2verbdep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "multiple_accuracies = ev_ml.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Sentence Learning\n",
      "Average Accuracy: 16.340694%\n",
      "STD: 0.211764\n"
     ]
    }
   ],
   "source": [
    "print \"Multiple Sentence Learning\"\n",
    "print \"Average Accuracy: %.6f%%\" % (np.mean(multiple_accuracies)*100)\n",
    "print \"STD: %.6f\" % np.std(multiple_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Evaluation Scheme 2\n",
    "\n",
    "* See above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.7 s, sys: 154 ms, total: 4.85 s\n",
      "Wall time: 4.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ev_ml2 = Evaluator_Multiple_Learning(parsed_sents, norm2verbdep, scheme=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "multiple_accuracies2 = ev_ml2.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Sentence Learning\n",
      "Average Accuracy: 48.895899%\n",
      "STD: 0.499878\n"
     ]
    }
   ],
   "source": [
    "print \"Multiple Sentence Learning\"\n",
    "print \"Average Accuracy: %.6f%%\" % (np.mean(multiple_accuracies2)*100)\n",
    "print \"STD: %.6f\" % np.std(multiple_accuracies2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. $\\alpha=0.1$ Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Single Sentence Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.71 s, sys: 223 ms, total: 4.93 s\n",
      "Wall time: 4.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ev_sg21 = Evaluator_Single_Learning(parsed_sents, ev_norms, v_argtype_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "single_accuracies21 = ev_sg21.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Sentence Learning\n",
      "Average Accuracy: 10.914826%\n",
      "STD: 0.196688\n"
     ]
    }
   ],
   "source": [
    "print \"Single Sentence Learning\"\n",
    "print \"Average Accuracy: %.6f%%\" % (np.mean(single_accuracies21)*100)\n",
    "print \"STD: %.6f\" % np.std(single_accuracies21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.86 s, sys: 192 ms, total: 5.05 s\n",
      "Wall time: 5.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ev_sg22 = Evaluator_Single_Learning(parsed_sents, ev_norms, v_argtype_pairs, scheme=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "single_accuracies22 = ev_sg22.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Sentence Learning\n",
      "Average Accuracy: 31.230284%\n",
      "STD: 0.463433\n"
     ]
    }
   ],
   "source": [
    "print \"Single Sentence Learning\"\n",
    "print \"Average Accuracy: %.6f%%\" % (np.mean(single_accuracies22)*100)\n",
    "print \"STD: %.6f\" % np.std(single_accuracies22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Multiple Sentence Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.25 s, sys: 184 ms, total: 4.44 s\n",
      "Wall time: 4.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ev_ml21 = Evaluator_Multiple_Learning(parsed_sents, norm2verbdep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "multiple_accuracies21 = ev_ml21.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Sentence Learning\n",
      "Average Accuracy: 16.340694%\n",
      "STD: 0.211764\n"
     ]
    }
   ],
   "source": [
    "print \"Multiple Sentence Learning\"\n",
    "print \"Average Accuracy: %.6f%%\" % (np.mean(multiple_accuracies21)*100)\n",
    "print \"STD: %.6f\" % np.std(multiple_accuracies21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.51 s, sys: 174 ms, total: 4.68 s\n",
      "Wall time: 4.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ev_ml22 = Evaluator_Multiple_Learning(parsed_sents, norm2verbdep, scheme=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "multiple_accuracies22 = ev_ml22.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Sentence Learning\n",
      "Average Accuracy: 48.895899%\n",
      "STD: 0.499878\n"
     ]
    }
   ],
   "source": [
    "print \"Multiple Sentence Learning\"\n",
    "print \"Average Accuracy: %.6f%%\" % (np.mean(multiple_accuracies22)*100)\n",
    "print \"STD: %.6f\" % np.std(multiple_accuracies22)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
